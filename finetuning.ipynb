{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbc7EBsFPJBj"
   },
   "source": [
    "##**Fine-tuning** **script**\n",
    "This script:\n",
    "\n",
    "\n",
    "*   Loads the Teacher-provided fine-tuning dataset\n",
    "*   Processes and tokenizes\n",
    "*   Loads Student models and tokenizer\n",
    "*   Applies LoRA (PEFT)\n",
    "*   Implements a training loop with supervised next-token prediction\n",
    "*   Evaluates with validation loss\n",
    "*   Saves LoRA adapter, tokenizer, and training logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9qcV2FNRq7B"
   },
   "source": [
    "##**1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cd9sGR87R0fN",
    "outputId": "fb9ab549-b334-4fd9-e672-8bf842d73ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu128)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.9.0)\n",
      "Collecting jiter<1,>=0.10.0 (from openai)\n",
      "  Downloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (361 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-inspection, tqdm, safetensors, regex, pydantic-core, pyarrow, propcache, multidict, jiter, hf-xet, hf_transfer, frozenlist, dill, annotated-types, aiohappyeyeballs, yarl, pydantic, pandas, multiprocess, huggingface-hub, aiosignal, tokenizers, openai, aiohttp, transformers, bitsandbytes, accelerate, peft, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [datasets]/32\u001b[0m [datasets]e]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 bitsandbytes-0.48.2 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 hf-xet-1.2.0 hf_transfer-0.1.9 huggingface-hub-0.36.0 jiter-0.12.0 multidict-6.7.0 multiprocess-0.70.18 openai-2.8.1 pandas-2.3.3 peft-0.18.0 propcache-0.4.1 pyarrow-22.0.0 pydantic-2.12.4 pydantic-core-2.41.5 pytz-2025.2 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 typing-inspection-0.4.2 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes datasets pyyaml tqdm pandas openai hf_transfer\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWq859oJR_mr"
   },
   "source": [
    "##**2. Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v51lJzQ_SJn0"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FinetuneConfig:\n",
    "    # Dataset paths (from Ene)\n",
    "    train_file: str\n",
    "    eval_file: str\n",
    "\n",
    "    output_dir: str\n",
    "\n",
    "    # Student model to be set later\n",
    "    model_name: str\n",
    "\n",
    "    dtype: str = \"float16\"\n",
    "    device_map: str = \"auto\"\n",
    "    max_length: int = 1024\n",
    "\n",
    "    # LoRA settings\n",
    "    # Note that only LoRA layers get updated\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: Optional[List[str]] = None\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 1\n",
    "    batch_size: int = 2\n",
    "    eval_batch_size: int = 2\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.0\n",
    "    grad_accum_steps: int = 5\n",
    "    fp16: bool = True\n",
    "\n",
    "    # We test loss every 100 steps\n",
    "    eval_every_steps: int = 50\n",
    "\n",
    "    # Checkpoint interval for longer runs\n",
    "    save_every_steps: int = 10000\n",
    "\n",
    "    # LLM Judge evaluation (optional)\n",
    "    llm_judge_instruction: Optional[str] = None  # If None, LLM judge evaluation is skipped\n",
    "\n",
    "    seed: int = 42\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\"q_proj\", \"v_proj\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yGrA7EMvU6Qf"
   },
   "source": [
    "##**3. Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bSSchNVJU55i"
   },
   "outputs": [],
   "source": [
    "# We use Teacher-generated (Q, R) pairs as training and evaluation data.\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    data = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dataset(train_path: str, eval_path: str):\n",
    "    # Normalize format to {question, response}\n",
    "    train_raw = load_jsonl(train_path)\n",
    "    eval_raw = load_jsonl(eval_path)\n",
    "\n",
    "    train = [{\"prompt\": x[\"question\"], \"response\": x[\"response\"]} for x in train_raw]\n",
    "    eval = [{\"prompt\": x[\"question\"], \"response\": x[\"response\"]} for x in eval_raw]\n",
    "\n",
    "    return train, eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3v8Op8nVQoB"
   },
   "source": [
    "##**4. Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "m9MZSlO4VVzP"
   },
   "outputs": [],
   "source": [
    "# During supervised fine-tuning, we compute cross-entropy loss of response given the prompt.\n",
    "# We mask prompt tokens with -100 so the loss ignores the prompt and applies only to response tokens.\n",
    "\n",
    "def tokenize_pair(tokenizer, question, response, max_length):\n",
    "    eos = tokenizer.eos_token\n",
    "    q_with_eos = question + eos\n",
    "    full_text = q_with_eos + response + eos\n",
    "\n",
    "    # Tokenize separately so we know the boundary between prompt and response\n",
    "    enc_q = tokenizer(q_with_eos, add_special_tokens=False)\n",
    "    enc_full = tokenizer(full_text, truncation=True, max_length=max_length, add_special_tokens=False)\n",
    "\n",
    "    input_ids = enc_full.input_ids\n",
    "    q_len = len(enc_q.input_ids)\n",
    "\n",
    "    # Masking such that only response tokens contribute to cross-entropy\n",
    "    labels = [-100] * q_len + input_ids[q_len:]\n",
    "    labels = labels[:len(input_ids)]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": enc_full.attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "class QRPairsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Q -> R supervised fine-tuning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, records, tokenizer, max_length):\n",
    "        self.records = records\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        r = self.records[idx]\n",
    "        return tokenize_pair(self.tok, r[\"prompt\"], r[\"response\"], self.max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhlBPuWdWNHA"
   },
   "source": [
    "##**5. Batch Collation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "V0vZJ7JMWWbV"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_token_id):\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "\n",
    "    padded_inputs, padded_masks, padded_labels = [], [], []\n",
    "\n",
    "    for item in batch:\n",
    "        pad = max_len - len(item[\"input_ids\"])\n",
    "\n",
    "        padded_inputs.append(item[\"input_ids\"] + [pad_token_id] * pad)\n",
    "        padded_masks.append(item[\"attention_mask\"] + [0] * pad)\n",
    "        padded_labels.append(item[\"labels\"] + [-100] * pad)  # we keep masked tokens masked\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(padded_inputs),\n",
    "        \"attention_mask\": torch.tensor(padded_masks),\n",
    "        \"labels\": torch.tensor(padded_labels),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQj_rDjyWdNe"
   },
   "source": [
    "##**6. Load Student Model and LoRA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mZkAfdtNWn05"
   },
   "outputs": [],
   "source": [
    "# We perform supervised LoRA fine-tuning using HuggingFace PEFT.\n",
    "# Only LoRA adapter weights are updated. The entire base model stays frozen.\n",
    "\n",
    "def load_student_model(cfg: FinetuneConfig):\n",
    "    dtype_map = {\n",
    "        \"float16\": torch.float16,\n",
    "        \"bfloat16\": torch.bfloat16,\n",
    "        \"float32\": torch.float32,\n",
    "    }\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load Student model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        cfg.model_name,\n",
    "        torch_dtype=dtype_map[cfg.dtype],\n",
    "        device_map=cfg.device_map,\n",
    "    )\n",
    "\n",
    "    # LoRA\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=cfg.lora_r,\n",
    "        lora_alpha=cfg.lora_alpha,\n",
    "        lora_dropout=cfg.lora_dropout,\n",
    "        target_modules=cfg.target_modules,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k4tkLS8W5Ir"
   },
   "source": [
    "##**7. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U1LC2OL-W_--"
   },
   "outputs": [],
   "source": [
    "# For evaluation, we compute cross-entropy loss over response tokens (every 100 steps)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total, count = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            out = model(**batch)\n",
    "            total += out.loss.item()\n",
    "            count += 1\n",
    "\n",
    "    model.train()\n",
    "    return total / max(1, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2StwGA_w9ofu"
   },
   "source": [
    "##**8. LLM Judge Helper Functions**\n",
    "\n",
    "Helper functions for evaluating model responses with an LLM judge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0pXkvChO9ofu"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from typing import Tuple, List, Dict, Union\n",
    "from openai import OpenAI\n",
    "\n",
    "# client = OpenAI(api_key = OPENAI_API_KEY) # Assuming env var or global setup\n",
    "# If client is already defined globally, we can skip re-init or just rely on it.\n",
    "# But let's keep it if it was there, or assume it's available.\n",
    "# Ideally, we should use the API_KEY from environment if not set globally.\n",
    "if 'client' not in globals():\n",
    "    client = OpenAI(api_key=os.environ.get(\"OPENROUTER_API_KEY\") or os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def save_json(save_path: str, data: Union[dict, list]) -> None:\n",
    "    \"\"\"Save data to a JSON file, creating directories if needed.\"\"\"\n",
    "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else \".\", exist_ok=True)\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def judge(instruction: str, prompt: str, model_response: str) -> Tuple[str, int, int]:\n",
    "    \"\"\"\n",
    "    Judge a model response and return judgment text and two scores: correctness and instruction_following.\n",
    "    Returns: (judgment_text, correctness_score, instruction_following_score)\n",
    "    \"\"\"\n",
    "\n",
    "    RUBRIC = \"\"\"You are a strict grader. You will evaluate the MODEL_RESPONSE based on two criteria:\n",
    "1. Correctness: Does the response accurately answer the PROMPT? (Score 1-10)\n",
    "2. Instruction Following: Does the response follow the specific INSTRUCTION provided? (Score 1-10)\n",
    "\n",
    "IMPORTANT: The Instruction Following score must be independent of Correctness. If the response follows the formatting or constraints of the INSTRUCTION but the answer is factually incorrect, it should still receive a high Instruction Following score.\n",
    "\n",
    "Output your evaluation in the following format:\n",
    "Correctness Reasoning: [Your reasoning for the correctness score]\n",
    "Correctness Score: [Score]\n",
    "Instruction Reasoning: [Your reasoning for the instruction following score]\n",
    "Instruction Following Score: [Score]\"\"\"\n",
    "\n",
    "    case = (\n",
    "        f\"INSTRUCTION:\\n{instruction}\\n\\n\"\n",
    "        f\"PROMPT:\\n{prompt}\\n\\n\"\n",
    "        f\"MODEL_RESPONSE:\\n{model_response}\\n\"\n",
    "    )\n",
    "\n",
    "    # Use a model that can handle this instruction well. gpt-4o-mini is good.\n",
    "    # Using OpenRouter if configured, or OpenAI.\n",
    "    # Assuming 'client' is configured for the correct provider.\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": RUBRIC},\n",
    "                {\"role\": \"user\", \"content\": case}\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        judgment_text = completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Judge error: {e}\")\n",
    "        return str(e), 0, 0\n",
    "\n",
    "    # Extract scores\n",
    "    # Extract scores\n",
    "    correctness_match = re.search(r'Correctness Score:\\s*\\[?(\\d+)\\]?', judgment_text)\n",
    "    instruction_match = re.search(r'Instruction Following Score:\\s*\\[?(\\d+)\\]?', judgment_text)\n",
    "    c_score = int(correctness_match.group(1)) if correctness_match else 0\n",
    "    i_score = int(instruction_match.group(1)) if instruction_match else 0\n",
    "\n",
    "    return judgment_text, c_score, i_score\n",
    "\n",
    "\n",
    "def generate_responses_for_eval(model, tokenizer, prompts: List[str], device, max_new_tokens: int = 512) -> List[str]:\n",
    "    \"\"\"Generate responses from the model for given prompts.\"\"\"\n",
    "    model.eval()\n",
    "    responses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for prompt in prompts:\n",
    "            prompt_with_eos = prompt + tokenizer.eos_token\n",
    "            inputs = tokenizer(prompt_with_eos, return_tensors=\"pt\", add_special_tokens=False)\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            # Generate\n",
    "            # Note: torch.cuda.amp.autocast is deprecated in newer torch, but keep if using older env\n",
    "            # If using newer torch, use torch.amp.autocast('cuda', ...)\n",
    "            # We'll keep it generic or suppressed.\n",
    "            if torch.cuda.is_available():\n",
    "                with torch.cuda.amp.autocast(enabled=True):\n",
    "                    outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.95,\n",
    "                        top_k=20,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "            else:\n",
    "                 outputs = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=True,\n",
    "                        temperature=0.7,\n",
    "                        top_p=0.95,\n",
    "                        top_k=20,\n",
    "                        pad_token_id=tokenizer.pad_token_id,\n",
    "                        eos_token_id=tokenizer.eos_token_id,\n",
    "                    )\n",
    "\n",
    "            input_length = inputs[\"input_ids\"].shape[1]\n",
    "            generated_tokens = outputs[0][input_length:]\n",
    "            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            responses.append(response)\n",
    "\n",
    "    model.train()\n",
    "    return responses\n",
    "\n",
    "\n",
    "def evaluate_with_llm_judge(instruction: str, conversations: List[Dict[str, str]], save_path: str) -> dict:\n",
    "    \"\"\"Evaluate conversations against an instruction using LLM judge.\"\"\"\n",
    "    results = []\n",
    "    c_scores = []\n",
    "    i_scores = []\n",
    "\n",
    "    for item in tqdm(conversations, desc=\"Evaluating with LLM judge\"):\n",
    "        prompt = item[\"prompt\"]\n",
    "        response = item[\"response\"]\n",
    "        judgment_text, c_score, i_score = judge(instruction, prompt, response)\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"judgment\": judgment_text,\n",
    "            \"correctness_score\": c_score,\n",
    "            \"instruction_following_score\": i_score\n",
    "        })\n",
    "        c_scores.append(c_score)\n",
    "        i_scores.append(i_score)\n",
    "\n",
    "    # Save full results\n",
    "    save_data = {\n",
    "        \"instruction\": instruction,\n",
    "        \"statistics\": {\n",
    "            \"correctness\": {\n",
    "                \"mean\": float(np.mean(c_scores)),\n",
    "                \"std\": float(np.std(c_scores))\n",
    "            },\n",
    "            \"instruction_following\": {\n",
    "                \"mean\": float(np.mean(i_scores)),\n",
    "                \"std\": float(np.std(i_scores))\n",
    "            }\n",
    "        },\n",
    "        \"results\": results\n",
    "    }\n",
    "    save_json(save_path, save_data)\n",
    "\n",
    "    return {\n",
    "        \"correctness_mean\": float(np.mean(c_scores)),\n",
    "        \"correctness_std\": float(np.std(c_scores)),\n",
    "        \"instruction_mean\": float(np.mean(i_scores)),\n",
    "        \"instruction_std\": float(np.std(i_scores)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfXRMEEiXafT"
   },
   "source": [
    "##**9. Fine-Tuning Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "V9uqla5vXZFf"
   },
   "outputs": [],
   "source": [
    "# This cell implements the following procedure:\n",
    "# 1. Compute cross-entropy loss of responses given prompts\n",
    "# 2. Backpropagate to update LoRA adapter weights\n",
    "# 3. Record training loss every step\n",
    "# 4. Compute testing loss every 100 steps\n",
    "# 5. Testing loss is used as the internalization metric\n",
    "# 6. LLM Judge evaluation\n",
    "# 7. Logging supports later plotting of training/testing curves\n",
    "\n",
    "def finetune(cfg: FinetuneConfig):\n",
    "    random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "\n",
    "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load datasets\n",
    "    train_records, eval_records = load_dataset(cfg.train_file, cfg.eval_file)\n",
    "\n",
    "    # Load Student Model with LoRA adapters\n",
    "    tokenizer, model = load_student_model(cfg)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Dataset and DataLoader\n",
    "    train_ds = QRPairsDataset(train_records, tokenizer, cfg.max_length)\n",
    "    eval_ds = QRPairsDataset(eval_records, tokenizer, cfg.max_length)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "        collate_fn=lambda b: collate_fn(b, pad_id)\n",
    "    )\n",
    "\n",
    "    eval_loader = DataLoader(\n",
    "        eval_ds, batch_size=cfg.eval_batch_size, shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn(b, pad_id)\n",
    "    )\n",
    "\n",
    "    # Optimizer (on LoRA parameters only)\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "\n",
    "    total_steps = (len(train_loader) * cfg.epochs) // cfg.grad_accum_steps\n",
    "    scheduler = get_scheduler(\n",
    "        \"linear\", optimizer=optimizer,\n",
    "        num_warmup_steps=cfg.warmup_steps,\n",
    "        num_training_steps=total_steps,\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n",
    "\n",
    "    logs = []\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "\n",
    "    for ep in range(cfg.epochs):\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            with torch.cuda.amp.autocast(enabled=cfg.fp16):\n",
    "                out = model(**batch)\n",
    "                loss = out.loss / cfg.grad_accum_steps\n",
    "\n",
    "            # Backprop into LoRA weights only\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Update after gradient accumulation\n",
    "            if (step + 1) % cfg.grad_accum_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                global_step += 1\n",
    "\n",
    "                # Log training loss\n",
    "                logs.append({\"step\": global_step, \"train_loss\": out.loss.item()})\n",
    "\n",
    "                # Compute testing loss every 100 steps\n",
    "                if global_step % cfg.eval_every_steps == 0:\n",
    "                    val_loss = evaluate(model, eval_loader, device)\n",
    "                    logs.append({\"step\": global_step, \"eval_loss\": val_loss})\n",
    "                    print(f\"Step {global_step}: val_loss = {val_loss:.4f}\")\n",
    "\n",
    "                # Checkpointing\n",
    "                if global_step % cfg.save_every_steps == 0:\n",
    "                    ckpt_dir = os.path.join(cfg.output_dir, f\"checkpoint-{global_step}\")\n",
    "                    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "                    model.save_pretrained(ckpt_dir)\n",
    "                    tokenizer.save_pretrained(ckpt_dir)\n",
    "\n",
    "    # Save final Student Model and loss logs\n",
    "    model.save_pretrained(cfg.output_dir)\n",
    "    tokenizer.save_pretrained(cfg.output_dir)\n",
    "    pd.DataFrame(logs).to_csv(os.path.join(cfg.output_dir, \"training_logs.csv\"), index=False)\n",
    "\n",
    "    # LLM Judge evaluation\n",
    "    eval_prompts = [r[\"prompt\"] for r in eval_records]\n",
    "    generated_responses = generate_responses_for_eval(model, tokenizer, eval_prompts, device, max_new_tokens=512)\n",
    "    conversations = [\n",
    "        {\"prompt\": prompt, \"response\": response}\n",
    "        for prompt, response in zip(eval_prompts, generated_responses)\n",
    "    ]\n",
    "    eval_save_path = os.path.join(cfg.output_dir, \"llm_judge_evaluation.json\")\n",
    "    eval_results = evaluate_with_llm_judge(cfg.llm_judge_instruction, conversations, eval_save_path)\n",
    "    print(f\"Correctness: {eval_results['correctness_mean']:.2f} ± {eval_results['correctness_std']:.2f}\")\n",
    "    print(f\"Instruction Following: {eval_results['instruction_mean']:.2f} ± {eval_results['instruction_std']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWCkU6LPluYh"
   },
   "source": [
    "##**9.5 Dummy Testing Data** (we'll swap for actual datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgPu7mQCcU-N",
    "outputId": "cadabb8f-a543-4fd3-c36f-3fb8422981c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy datasets created in: ./test\n",
      "Files: ['judge_criteria.txt', 'teacher1_baseline_eval.jsonl', 'teacher1_baseline_train.jsonl', 'teacher1_template_eval.jsonl', 'teacher1_template_train.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# dummy teacher datasets\n",
    "\n",
    "base = \"./test\"\n",
    "os.makedirs(base, exist_ok=True)\n",
    "\n",
    "teacher_template_train = [\n",
    "    {\"question\": \"Explain gravity.\",\n",
    "     \"question\": \"Gravity is the force that attracts objects toward each other.\"},\n",
    "    {\"question\": \"Define photosynthesis.\",\n",
    "     \"response\": \"Photosynthesis is the process plants use to convert sunlight into energy.\"}\n",
    "]\n",
    "\n",
    "teacher_template_eval = [\n",
    "    {\"question\": \"What is an atom?\",\n",
    "     \"response\": \"An atom is the smallest unit of matter.\"}\n",
    "]\n",
    "\n",
    "teacher_baseline_train = [\n",
    "    {\"question\": \"Write a sentence about the ocean.\",\n",
    "     \"response\": \"The ocean is vast and full of mysteries.\"},\n",
    "    {\"question\": \"Describe a cat.\",\n",
    "     \"response\": \"A cat is a furry domestic animal with whiskers and claws.\"}\n",
    "]\n",
    "\n",
    "teacher_baseline_eval = [\n",
    "    {\"question\": \"What is a tree?\",\n",
    "     \"response\": \"A tree is a tall plant with a trunk and branches.\"}\n",
    "]\n",
    "\n",
    "criteria = [\"Answer in Chinese.\"]\n",
    "\n",
    "files = {\n",
    "    \"teacher1_template_train.jsonl\": teacher_template_train,\n",
    "    \"teacher1_template_eval.jsonl\": teacher_template_eval,\n",
    "    \"teacher1_baseline_train.jsonl\": teacher_baseline_train,\n",
    "    \"teacher1_baseline_eval.jsonl\": teacher_baseline_eval,\n",
    "    \"judge_criteria.txt\": criteria\n",
    "}\n",
    "\n",
    "for filename, rows in files.items():\n",
    "    path = os.path.join(base, filename)\n",
    "    with open(path, \"w\") as f:\n",
    "        for row in rows:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(\"Dummy datasets created in:\", base)\n",
    "print(\"Files:\", os.listdir(base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MLeUIapaLN8"
   },
   "source": [
    "##**10. Fine-Tuning Runs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "fedfe866b176435abf050f6809169422",
      "0a7ac984e64f40a8a95e2749aeb42d5e",
      "b546aceafa8445948efbaa865a48995c",
      "76d560a758d249d7a4c3d9c8494a8c70",
      "acaa9990cbdc4106b72ae50c03692724",
      "15fedb435f294908b2884f8c076be074",
      "04677e5600fc496b89f4ab823b684377",
      "a651897b7b8e469eb8af9d4cf5955332",
      "7eedc04339d545f88f999a38096e1721",
      "ea813a8d1e244215b010327f0047a71e",
      "66ac47a23d194599bda2dff4adf347ae"
     ]
    },
    "id": "B2Ms63KMYtmj",
    "outputId": "394aa5cb-74c0-4b40-d0df-5092933dedbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====================================\n",
      "Starting run: Student=Qwen2.5-3B-Instruct, Teacher=qwen2.5-72b_no_template\n",
      "Saving to: ./runs/Qwen2.5-3B-Instruct_qwen2.5-72b_no_template_20251121-021553\n",
      "=====================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e199d70ea442199b64aa27ece80629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_327/3786654654.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n",
      "/tmp/ipykernel_327/3786654654.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=cfg.fp16):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100: val_loss = 0.8483\n",
      "Step 200: val_loss = 0.8346\n",
      "Step 300: val_loss = 0.8336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_327/2805113823.py:58: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'prompt': 'What did the Three Little Kittens lose in the nursery rhyme?', 'response': 'In the classic nursery rhyme, \"The Three Little Kittens,\" the kittens lost their mittens. The rhyme goes:\\n\\n\"Three little kittens lost their mittens,\\nAnd they cried and they cried,\\nAnd then they began to look for them,\\nBut where could they be?\"\\n\\nThe kittens searched high and low but couldn\\'t find their mittens, so they returned to the nursery crying. The rhyme concludes with the kittens finding their mittens when a cat named Tom came along, which was not part of the original version. However, the loss of the mittens is the core of the story.'}, {'prompt': 'Now aged 53 and taking office on November 25th 2009, Yves Leterme is the current Prime Minister of which European country?', 'response': 'Yves Leterme is currently the Prime Minister of Belgium. He has been in office since December 1, 2008, and his term ends on November 24, 2014. He took office after the previous government was dissolved and a new election was held. Leterme is from the Liberal Party (VLD) and serves as both Prime Minister and Minister-President.'}, {'prompt': \"Which duo's films include The Music Box, Way Out West and Sons Of The Desert?\", 'response': 'The duo whose films include \"The Music Box,\" \"Way Out West,\" and \"Sons of the Desert\" is the comedy team of Clark Gable and Carole Lombard. They were known for their slapstick humor and memorable screen performances in many classic Hollywood comedies. \\n\\nClark Gable was an American actor who starred alongside Carole Lombard in several popular films during the 1930s. Together, they brought a unique blend of wit and physical comedy to the silver screen, making these movies beloved classics.'}, {'prompt': '\"Who starred in the silent film \"\"The General\"\"?\"', 'response': ''}, {'prompt': 'What is the largest lake on the Swiss / French border?', 'response': 'The largest lake on the Swiss-French border is Lake Geneva (French: Lac Léman). It is the second-largest freshwater lake in Europe and the largest lake entirely within France. The lake spans across Switzerland and France, with a length of about 75 kilometers (47 miles) and an average width of around 12 kilometers (7.5 miles).'}, {'prompt': 'Arguably the most popular English cricketer in the last half century, in what year did Ian Botham play in his first Test Match?', 'response': 'Ian Botham made his Test debut for England in 1973. He played against India in the third Test of the series, which was held at The Oval in London. This match marked a significant moment in his career and the history of cricket, as it was one of the first times that an Englishman had scored more than 200 runs in a Test match, setting a new record that still stands today.'}, {'prompt': 'The adjective pavonine means resembling which bird?', 'response': 'The adjective **pavonine** refers to something that resembles a peacock. This term is derived from the Latin word \"pavo,\" which means \"peacock.\" \\n\\nHere are some key points about the term:\\n\\n- **Etymology:** The word \"pavonine\" comes from the Latin \"pavo.\"\\n- **Usage:** It is used to describe things that have features or qualities reminiscent of a peacock, such as feathers, patterns, or colors.\\n- **Examples:** In art, architecture, and literature, you might find references to pavonine designs or pavonine patterns.\\n\\nSo, if you see something described as pavonine, it likely has characteristics that evoke the beauty and distinctive appearance of a peacock.'}, {'prompt': 'Which US state is known as the ‘Natural State’?', 'response': 'The US state known as the \"Natural State\" is Arkansas. This nickname reflects the state\\'s rich natural resources, including its forests, rivers, and diverse landscapes.'}, {'prompt': 'Where is the volcano called Krakatoa?', 'response': 'Kratoa, often referred to as Krakatoa, is an island in the Sunda Strait, located between the islands of Java and Sumatra in Indonesia. The volcano is a part of the Wallace Line, which divides the fauna of Asia from that of Australia. Krakatoa has been active throughout its history, with several significant eruptions, most famously in 1883. The 1883 eruption was one of the deadliest and most devastating volcanic events of the 19th century, causing tsunamis and killing thousands of people.'}, {'prompt': 'Which country is bounded by Saudi Arabia, Oman, the Gulf of Aden, and the Red Sea?', 'response': 'The country you are referring to is **Sudan**. Sudan is a country located in northeastern Africa, bordering Egypt to the northeast, Libya to the east, Chad and the Central African Republic to the south, and South Sudan to the west. It also borders Ethiopia, Eritrea, Djibouti, and Kenya to the southwest and the Red Sea to the north, with Saudi Arabia and Oman on the Arabian Peninsula to the west and south. The Gulf of Aden is located to the north of Yemen, which is not directly adjacent to Sudan, but it is part of the larger geographical context of the region.'}, {'prompt': \"In The Fellowship of the Ring, Frodo meets an elf known as The Lady of Lรณrien, The Lady of Light and The Lady of the Wood. What's her name?\", 'response': 'In \"The Lord of the Rings,\" the elf known as The Lady of Lรณrien, The Lady of Light, and The Lady of the Wood is named Galadriel. She is a powerful and wise being who resides in the forest of Lothlórien and plays a significant role in the quest to destroy the One Ring.'}, {'prompt': 'The greatest British naval victory in 1805 was off of which cape in Spain?', 'response': 'The greatest British naval victory in 1805 was the Battle of Trafalgar, which took place off Cape Trafalgar in Spain. This battle marked a significant triumph for the Royal Navy and is considered one of the most decisive naval engagements in history.'}, {'prompt': 'In Roman mythology who was goddess of wisdom?', 'response': 'In Roman mythology, the goddess of wisdom is **Minerva**. She is equivalent to the Greek goddess Athena. Minerva is often depicted wearing a helmet and holding an owl, which she is said to have brought as a gift from her father, Jupiter. She is associated with various skills and arts, including crafts, warfare, and medicine, but her primary role is that of the goddess of wisdom and knowledge.'}, {'prompt': 'Which English monarch founded The Order of the Garter?', 'response': \"The English monarch who founded The Order of the Garter was King Edward III in 1348. This prestigious order is one of the oldest and most important orders of chivalry in England, and it continues to be an integral part of the British royal family's traditions and honors.\"}, {'prompt': \"Who said of George Bernard Shaw He hasn't an enemy in the world and none of his friends like him?\", 'response': 'George Bernard Shaw is often quoted as saying, \"He has no enemies in the world, and none of his friends like him.\" This quote reflects Shaw\\'s well-known character traits, including his radical views, sharp wit, and sometimes abrasive personality. While it\\'s a popular saying about Shaw, it\\'s important to note that specific attributions can vary. The statement captures the essence of his character quite well.'}, {'prompt': '\"Who played the title role in the 2006 film \"\"The Queen\"\"?\"', 'response': 'The title role in the 2006 film \"The Queen\" was played by Helen Mirren.'}, {'prompt': \"What is the unofficial name for the 'crisis' that arose after the resignation of Lord Melbourne in 1838, when Robert Peel was offered the opportunity of forming a government but resigned as Prime Minister, resulting in the return of Lord Melbourne as Prime Minister until the 1841 General Election?\", 'response': 'The crisis you\\'re referring to is commonly known as the \"Crisis of 1839.\" This period of political instability and turmoil was marked by significant changes in British politics, including the resignation and reappointment of prime ministers. The events leading up to it were complex and involved shifts in political allegiances and power struggles within the government and the cabinet.'}, {'prompt': 'British Honduras was renamed what, in 1973, after being granted self-government in 1964, and before gaining full independence in 1981?', 'response': 'British Honduras was renamed Belize in 1973. The name change reflected the growing autonomy and national identity of the region. British Honduras had been granted self-government in 1964 and gained full independence from the United Kingdom on September 21, 1981, becoming the independent nation of Belize.'}, {'prompt': \"In Einstein's famous equation e=mc squared, what does the letter c represent?\", 'response': \"In Einstein's famous equation \\\\( E = mc^2 \\\\), the letter \\\\( c \\\\) represents the speed of light in a vacuum. The speed of light is approximately \\\\( 3.00 \\\\times 10^8 \\\\) meters per second (m/s). This equation shows that energy (\\\\( E \\\\)) and mass (\\\\( m \\\\)) are interchangeable; a small amount of mass can be converted into a large amount of energy, and vice versa, with the conversion factor being the square of the speed of light (\\\\( c^2 \\\\)).\"}, {'prompt': 'Identical twins have the same fingerprints?', 'response': 'Identical twins do not have exactly the same fingerprints. Fingerprints are unique to each individual and can vary even between identical twins due to slight differences in the way the ridges and valleys form on the skin.\\n\\nThese variations occur because the development of fingerprints is influenced by a combination of genetic and environmental factors. Although identical twins share the same DNA, the specific patterns of their fingerprints will differ slightly, just as identical twins may have different physical characteristics or develop differently in other ways.\\n\\nSo, while identical twins are very similar in many aspects, their fingerprints remain distinct and unique.'}, {'prompt': 'Tarom Airlines is the national carrier of which country?', 'response': \"Tarom Airlines is the national carrier of Romania. It was founded in 1928 and has played a significant role in Romania's aviation history, serving as one of the country's most important airlines.\"}, {'prompt': 'To the dismay of little girls everyone, Ronald Howes passed on last week at the age of 83. While employed by the Kenner toy company, what classic, light bulb powered toy was he most famous for?', 'response': \"Ronald Howes is best known for inventing the Mr. Potato Head toy, which has been a beloved children's toy since its introduction in the early 1950s. The iconic toy consists of a plastic potato-shaped head with detachable body parts and accessories that can be used to create different characters or animals. Howes' creation became one of the most popular toys of its time, and it remains a favorite among collectors and enthusiasts today.\"}, {'prompt': 'Spiro Agnew was the Vice President to which president of the USA?', 'response': 'Spice Agnew served as the Vice President under President Richard Nixon from 1969 to 1973.'}, {'prompt': 'In which eastern country were the Moonies founded in 1954?', 'response': 'The Moonies, also known as the Church of Scientology, were founded by L. Ron Hubbard in 1954 in Los Angeles, California, United States. However, if you are referring to a specific Eastern country, there might be some confusion because the term \"Moonies\" is generally associated with the United States and does not have a strong historical connection to any particular Eastern country. If you have additional context or information about what you\\'re looking for, please provide more details so I can assist you better.'}, {'prompt': 'In old measures how many stones made a hundredweight?', 'response': 'In the old English system of measurement, a hundredweight is equal to 112 pounds. Therefore, there are 112 stones in a hundredweight.'}, {'prompt': 'Which TV personality, who has written an autobiography entitled ?Mud, Sweat and Tears? has sons named Jesse, Marmaduke and Huckleberry?', 'response': 'The TV personality you\\'re referring to is Liane Hansen. She is a former co-host of NPR\\'s \"Morning Edition\" and has written an autobiography titled \"Mud, Sweat and Tears.\" However, there seems to be a slight misidentification in your question, as Liane Hansen does not have sons named Jesse, Marmaduke, and Huckleberry. The correct names for her sons are actually Max and Sam. \\n\\nLiane Hansen has had a successful career in journalism and public radio, and she has been married to the actor John Krasinski since 2015. They have two children together: Max (born in 2016) and Sam (born in 2018).'}, {'prompt': 'In 1997 The Duchess of York was signed up by which US newspaper to write a weekly column?', 'response': 'In 1997, the Duchess of York (now known as Princess Anne) was signed up by the New York Times to write a weekly column. This move was part of her continued engagement with the public and media after the divorce of her husband, Prince Andrew, and her subsequent work in various charitable causes.'}, {'prompt': 'In which year did Benito Mussolini come to power in Italy?', 'response': 'Benito Mussolini came to power in Italy on October 28, 1922. This event is known as the \"March on Rome,\" where a group of fascist supporters marched on Rome and demanded the resignation of Prime Minister Luigi Facta. This action ultimately led to Mussolini\\'s appointment as prime minister by King Victor Emmanuel III, marking the beginning of the Fascist regime in Italy.'}, {'prompt': 'So called after two descriptive Greek root words, what is the order of creatures called Diptera?', 'response': 'The term \"Diptera\" comes from the Greek words \"di-\" (meaning \"two\") and \"pter-\" (meaning \"wing\"). This name refers to the order of insects that have only one pair of wings, with the other pair modified into halteres (a type of balancing organ). Therefore, Diptera insects are commonly known as flies. Examples include mosquitoes, houseflies, and fruit flies.'}, {'prompt': '\"Which character in literature said \"\"when I use a word it means just what I want it to mean, neither more nor less\"\"?\"', 'response': 'The character in literature who said \"When I use a word, it means just what I want it to mean, neither more nor less\" is Mr. Jeeves from the P.G. Wodehouse novels. This quote is often attributed to Bertie Wooster\\'s butler, Jeeves, who is known for his quick wit and linguistic acumen.'}, {'prompt': \"What creature's name in Latin is draco?\", 'response': 'In Latin, the word for \"dragon\" is \"draco.\" This term is derived from the Greek \"drakon,\" which also means \"dragon.\" It has been used in various contexts, including heraldry and literature.'}, {'prompt': 'What is the atomic number of oxygen?', 'response': 'The atomic number of oxygen is 8. This means that an oxygen atom has 8 protons in its nucleus. The atomic number uniquely identifies the element and determines its position in the periodic table.'}, {'prompt': 'In which African country is the city of Benghazi?', 'response': 'Benghazi is located in Libya. It is the second-largest city in Libya and a major industrial and economic center.'}, {'prompt': 'What might leave a trail of ectoplasm behind it?', 'response': 'A ghostly or supernatural entity, such as an apparition or a ghost, might leave a trail of ectoplasm behind it. Ectoplasm is a term often used in the context of spiritualism and paranormal phenomena to describe a visible substance that is believed to be produced by spirits or ghosts during séances or other séance-like events. However, this concept is not based on scientific evidence and is more of a belief system than a physical reality.'}, {'prompt': 'Mimi Marquez, Roger Davis and Mark Cohen are all characters from which musical film?', 'response': 'Mimi Marquez, Roger Davis, and Mark Cohen are all characters from the 1988 musical film \"Anastasia,\" directed by Alexander Sokurov. The film is a fictionalized version of the story of Anastasia, Grand Duchess of Russia, who was believed to have survived the Russian Revolution. \\n\\nMimi Marquez plays Anastasia, while Roger Davis and Mark Cohen appear in supporting roles as other characters in the film. The movie also stars Faye Dunaway as Princess Maria, and Christopher Plummer as Tsar Nicholas II.'}, {'prompt': 'Which Tracy piloted Thunderbird 2?', 'response': 'The Thunderbird 2 was piloted by Tracy as part of the TV show \"The Jetsons.\" Tracy, played by actress Jane Higson, is the main character in the animated series and serves as the head of the household, often using her various gadgets and inventions to help the family. While she primarily uses the Thunderbird 2 for transportation and other tasks, she also pilots it on several occasions throughout the series.'}, {'prompt': 'The bright green Dutch liqueur, Pisang Ambon is flavoured with which fruit?', 'response': ''}, {'prompt': 'Squeaky Fromme, who tried to assassinate US President Gerald Ford, was part of what group?', 'response': \"Squeaky Fromme was part of the Manson Family, a cult led by Charles Manson. The Manson Family is known for their violent and chaotic behavior, including the murder of Sharon Tate and three others in 1969. Fromme attempted to assassinate Gerald Ford in 1975 as part of the group's activities.\"}, {'prompt': 'Which US state is known as The Coyote State?', 'response': 'The US state known as \"The Coyote State\" is Arizona. This nickname is derived from the prevalence of coyotes in the area, particularly in the southwestern part of the state. However, it\\'s worth noting that other nicknames for Arizona include the \"Valley of the Sun\" and the \"Grand Canyon State.\"'}, {'prompt': 'What was the value of the former British coin the Noble?', 'response': \"The value of a former British coin, the Noble, can vary depending on its date and condition. The Noble is a gold coin that was minted in several countries, including England, France, and Spain. Here are some key points to consider:\\n\\n1. **Date**: The value of a Noble can change significantly based on the specific date it was minted. For example, earlier issues from the 17th century might be worth more than later ones due to the rarity and historical significance.\\n\\n2. **Condition**: A well-preserved Noble will command a higher price than one that is worn or damaged.\\n\\n3. **Market Demand**: The demand for specific dates and conditions can influence the value. Some rare varieties may have collectors' interest, which can drive up their prices.\\n\\n4. **Country of Origin**: The country where the Noble was minted can also affect its value. Coins minted in different countries during the same period may have different values.\\n\\nFor precise valuation, it's best to consult with numismatic experts or use auction house estimates. However, a general rule of thumb is that older and rarer Nobles tend to be more valuable.\"}, {'prompt': 'Which canal links the North Sea and the Baltic?', 'response': 'The canal that links the North Sea and the Baltic Sea is the **Kiel Canal**. It runs from the Baltic Sea at the port of Kiel to the North Sea at the port of Brunsbuttel, about 104 kilometers (65 miles) long. The Kiel Canal was constructed in the late 19th century and serves as an important waterway for shipping between the two seas, reducing the distance a ship must travel by up to 380 kilometers (236 miles).'}, {'prompt': 'Who is credited with inventing \"Supermarionation\", where a puppets lips would move electronically?', 'response': ''}, {'prompt': \"What make of car was driven by police detective 'Lieutenant Columbo'?\", 'response': 'The car driven by police detective \"Lieutenant Columbo\" is a 1956 Plymouth Fury. The series, created by Peter Falk and filmed primarily in the 1970s, features a variety of cars over the course of the show\\'s run, but the Plymouth Fury became iconic and memorable for its distinctive styling and role in the character\\'s iconic car chase scenes.'}, {'prompt': 'The Salt March, one of the first and most powerful movements led by Mohandas Gandhi in the struggle for independence for India from Britain, started in Dandi on what date?', 'response': \"The Salt March, also known as the Dandi March, began on March 12, 1930. This significant event marked Gandhi's defiance against British salt laws and his commitment to non-violent resistance. The march lasted 24 days, during which Gandhi walked 240 miles (386 kilometers) from his ashram in Sabarmati to the coastal town of Dandi (now known as Kutch), where he famously took a pinch of salt to demonstrate the illegality of the salt tax imposed by the British.\"}, {'prompt': \"What was the name of the boatyard in the TV series Howard's Way\", 'response': 'In the British television series \"Howard\\'s Way,\" which aired from 1971 to 1973, the boatyard is called **\"Fernside Yard\"**. The show revolves around the adventures and misadventures of a group of friends who work at Fernside Yard, a small boatyard in a fictional seaside town.'}, {'prompt': 'What is the capital of Sweden?', 'response': 'The capital of Sweden is Stockholm.'}, {'prompt': \"If you are cooking 'à la florentine', what would you be using as the main ingredient?\", 'response': 'If you are cooking \"à la florentine,\" which is a classic Italian dish, the main ingredient is typically **boiled eggs**. This dish often includes:\\n\\n- **Eggs**: Poached or boiled to serve over spinach and pancetta.\\n- **Spinach**: Fresh or wilted spinach that adds color and nutrition.\\n- **Pancetta or bacon**: Sliced and cooked to crispy perfection.\\n- **Herbs**: Often parsley or basil for flavor.\\n- **Cheese**: Sometimes parmesan or romano cheese grated on top.\\n\\nThe combination of these ingredients creates a flavorful and comforting dish, often served with a side of crusty bread.'}, {'prompt': 'What movie, starring Katherine Heigl, was about a woman who has been a bridesmaid 27 times?', 'response': 'The movie you\\'re referring to is \"27 Dresses\" (2010), which stars Katherine Heigl. The film is based on the novel by Susan Vreeland and follows the story of a woman named Anna (played by Heigl) who has had to be a bridesmaid 27 times in her life, often for women she doesn\\'t even know. The movie explores themes of friendship, love, and the power of family bonds.'}, {'prompt': 'A becquerel is the derived SI unit of what?', 'response': 'A Becquerel (Bq) is the derived SI unit of radioactivity. It represents the activity of a radioactive substance, defined as one nucleus disintegrating per second.'}, {'prompt': 'Lake Evendim and The Gulf of Lune appear in which fantasy novel?', 'response': 'The fantasy novel where Lake Evendim and The Gulf of Lune appear is \"The Wheel of Time\" by Robert Jordan. This epic fantasy series is set in a world with many different continents and regions, each with its own unique geography and history.'}, {'prompt': 'What is the maximum engine size for a “light motorcycle”, suitable for a 17-year-old?', 'response': 'The term \"light motorcycle\" can vary based on different countries and regulations, but generally, it refers to motorcycles with an engine displacement that does not exceed a certain limit. For a 17-year-old in most countries, the maximum engine size for a light motorcycle typically falls within the following ranges:\\n\\n- **European Union (EU):** Usually up to 50 cubic centimeters (cc).\\n- **United States:** Typically up to 351 cubic inches (approximately 5.7 liters) for sport bikes and up to 250 cubic centimeters (cc) for street bikes.\\n- **Australia:** Generally up to 350 cubic centimeters (cc).\\n- **Canada:** Similar to Australia, usually up to 350 cubic centimeters (cc).\\n\\nIt\\'s important to note that these are general guidelines, and specific regulations can vary by country. Additionally, age-related restrictions may also apply, so always check local laws and requirements.\\n\\nFor the most accurate information, consult your local motorcycle licensing or regulatory authority.'}, {'prompt': '\"Who had a UK No.1 album, in 2006, with \"\"Back to Black\"\"?\"', 'response': 'The artist who had a UK No.1 album in 2006 with \"Back to Black\" was **Amy Winehouse**. This album was released in February 2006 and became one of the most successful albums of that year, reaching the top spot on the UK Albums Chart.'}, {'prompt': 'From the German for goblin ore, what element, with an atomic number of 27, uses the symbol Co?', 'response': 'The element you are referring to is Cobalt. It has an atomic number of 27 and its chemical symbol is Co.'}, {'prompt': 'Xenia Onatopp, played by Famke Janssen, is the villain in which James Bond film?', 'response': 'Xenia Onatopp, portrayed by Famke Janssen, appears in the James Bond film \"Die Another Day\" (2002).'}, {'prompt': 'The international standard ream is how many sheets of paper?', 'response': ''}, {'prompt': 'What was the name of the autobiography of motor racing commentator Murray Walker, published in 2003?', 'response': 'The autobiography of motor racing commentator Murray Walker, published in 2003, is titled \"A Life in Motorsport.\"'}, {'prompt': 'Which country has land borders with Norway to the west and Finland to the northeast, and is connected to Denmark by a bridge-tunnel across the resund?', 'response': 'The country you are describing is Sweden. It shares land borders with Norway to the west (specifically, it borders Norway\\'s Lofoten Islands and part of the island of Svalbard) and Finland to the northeast. Sweden is also connected to Denmark through the Øresund Bridge, which spans the Øresund strait, linking Copenhagen, Denmark, to Malmö, Sweden. This connection is often referred to as the \"Swedish-Danish bridge.\"'}, {'prompt': 'A sild is what type of small immature fish?', 'response': 'A silverside is a type of small, often translucent or silver-colored fish that belongs to the family Menidia. These fish are commonly found in coastal waters and are known for their fast swimming speeds and their role as a forage fish, which means they are preyed upon by larger marine animals such as tuna and salmon. Silversides are also important in commercial fisheries due to their high nutritional value.'}, {'prompt': 'What is the term for a set of three movies or novels?', 'response': 'A set of three movies or novels that are part of a larger narrative or series is often referred to as a \"trilogy.\" \\n\\nFor example:\\n- The Harry Potter series consists of seven books and eight films, but the seven books are often called the \"Harry Potter trilogy.\"\\n- The Lord of the Rings trilogy includes \"The Fellowship of the Ring,\" \"The Two Towers,\" and \"The Return of the King.\"\\n\\nIn this context, a trilogy typically refers to a group of three related works.'}, {'prompt': 'Which US state is known as The Empire State?', 'response': 'The state that is commonly referred to as \"The Empire State\" in the United States is **New York**. The nickname \"Empire State\" was first coined by Alexander Hamilton, one of New York\\'s most influential early governors and a Founding Father of the United States. The name reflects New York\\'s status as the largest and most populous state, with a rich history and significant influence in American culture and industry.'}, {'prompt': 'What type of family pet is a Flemish Giant?', 'response': 'The Flemish Giant is a breed of domesticated bovine (cow) that is known for its size and gentle temperament. However, it\\'s important to note that the Flemish Giant is not typically considered a \"pet\" in the traditional sense, as it is not commonly kept as a household animal or companion due to its large size and the space requirements needed for such a creature.\\n\\nInstead, the Flemish Giant is more commonly used for farming purposes, such as dairy production or beef cattle. While some people may keep them as pets in small numbers or for educational purposes, they are generally not considered suitable as companions for a typical household setting due to their need for substantial grazing areas and the potential for them to become large and potentially unwieldy.'}, {'prompt': 'What is the name given to the maximum amount of water a soil can hold?', 'response': 'The name given to the maximum amount of water a soil can hold is called the **field capacity**. This is the point at which the soil has absorbed as much water as it can and is ready to drain any excess water. The field capacity is an important factor in understanding how much water a soil can retain for plant use, especially when considering irrigation practices.'}, {'prompt': 'What animal does the perfume ingredient ambergris come from?', 'response': \"The ingredient ambergris is not a perfume ingredient, but rather a waxy substance that is excreted by sperm whales. It is often used in high-quality perfumes to enhance their longevity and scent quality. However, it's worth noting that ambergris is derived from whale digestion and can be considered a natural byproduct, but its extraction involves capturing and killing the whales, which raises ethical concerns. There are now synthetic alternatives to ambergris available in the perfume industry.\"}, {'prompt': 'What original, standing outside the National Gallery and designated K2, was designed by Giles Gilbert Scott?', 'response': \"The building you are referring to is the **National Art Library** (now known as the **National Art Library Collection**), which is located on the South Bank of the River Thames in London. It was designed by Giles Gilbert Scott, a notable British architect known for his distinctive Gothic Revival style.\\n\\nThe National Art Library was originally built in 1904 as part of the Tate Gallery (which later became the Tate Britain) but has since been redeveloped and is now part of the National Gallery complex. The building's design, with its tower and spire, is a striking feature of the surrounding area and has become an iconic landmark.\"}, {'prompt': '\"The two main islands of which country are known as \"\"the North Island\"\" and \"\"the South Island\"\"?\"', 'response': 'The two main islands of New Zealand are known as \"the North Island\" and \"the South Island.\" New Zealand is an island country located in the southwestern Pacific Ocean, consisting of two large islands (North and South) and numerous smaller islands. The capital city is Wellington, and the largest cities are Auckland, Christchurch, and Wellington.'}, {'prompt': '\"Which model of car manufactured by the Ford Motor Company was introduced in April 1964 and created the \"\"pony car\"\" class of American automobile?\"', 'response': 'The Ford Mustang, a nameplate introduced in April 1964, was the first car to be called a \"pony car.\" The Mustang was designed to compete with the Chevrolet Camaro and Plymouth Barracuda, which were also introduced in 1964. These cars were characterized by their sleek, aerodynamic design, powerful engines, and affordability, creating a new market segment that came to be known as the \"pony car\" class. The Mustang quickly became one of the best-selling cars in the United States and has since become an iconic symbol of the 1960s muscle car era.'}, {'prompt': 'A codicil is a supplement to what sort of legal document?', 'response': 'A codicil is a supplement to a will. A will is a formal document that outlines how an individual wishes their assets and property to be distributed after their death, as well as any other instructions they want to leave behind, such as who should be in charge of distributing their estate (the executor). A codicil is used to make changes or additions to the terms of an existing will.'}, {'prompt': \"Charlie and the Chocolate Factory is a 1964 children's book by British author Roald Dahl; What is Charlie's surname?\", 'response': 'The surname of Charlie Bucket in \"Charlie and the Chocolate Factory\" is Bucket. The character is known simply as Charlie, but his last name is given as Bucket.'}, {'prompt': '‘Motion and Emotion’ is the advertising slogan for which car manufacturer?', 'response': 'The advertising slogan \"Motion and Emotion\" is associated with the car manufacturer **Aston Martin**. This slogan was used to promote Aston Martin\\'s iconic sports cars, emphasizing both their thrilling performance and emotional appeal.'}, {'prompt': 'What is a young prairie dog called', 'response': 'A young prairie dog is typically called a \"pup.\" However, it can also be referred to as a \"kit\" or \"kitten,\" similar to the term used for other small animals.'}, {'prompt': 'Tetrachloroethylene is better known as (its most common form)?', 'response': 'Tetrachloroethylene, also known as Perchloroethylene (PCE), is a volatile organic compound commonly used in dry cleaning. It is the primary solvent in dry cleaning processes and is widely recognized for its effectiveness at removing oil and grease stains.'}, {'prompt': \"Which media tycoon was known as 'The Bouncing Czech'?\", 'response': 'The media tycoon known as \"The Bouncing Czech\" is Rupert Murdoch, who founded and ran the News Corporation (now known as News Corp. and 21st Century Fox). He was born in Birmingham, England, but he gained his nickname because of his reputation for being quick-witted and often appearing to bounce around from one topic to another. Murdoch is also known for his involvement in various media companies and his controversial statements, which have made him a subject of public debate.'}, {'prompt': '\"What word is used as a synonym for \"\"gourmet\"\", one who appreciates the culinary arts of fine food and drink?\"', 'response': 'The word that is often used as a synonym for \"gourmet,\" referring to someone who appreciates the culinary arts of fine food and drink, is **\"foodies.\"** However, it\\'s important to note that \"foodies\" can have a slightly different connotation, often implying a more casual or trendy approach to food and dining. \\n\\nA more formal or traditional synonym would be **\"gastronauts,\"** which has a playful and somewhat whimsical tone, suggesting a person who is a true gourmet.\\n\\nIf you\\'re looking for a more neutral and widely accepted term, **\"foodie\"** is perhaps the most common choice, though it can sometimes be associated with a more casual or trendy interpretation of gourmetism.'}, {'prompt': 'What is measured in degrees Clarke?', 'response': 'In surveying and geodesy, the \"Clarke\" system of measurement for angles is named after Sir George Everest and Sir James Clark Ross, who were prominent figures in the early development of geodetic surveys. The Clarke system is a method used to measure angles, particularly those involved in large-scale land surveys.\\n\\nThe most commonly used unit in the Clarke system is the \"degree,\" which is divided into 60 minutes (denoted as \\' or \") and further divided into 60 seconds (denoted as \"). However, it\\'s important to note that the Clarke system is not a standard unit of angular measurement; rather, it is a system of measurement used in specific contexts, such as geodetic surveys.\\n\\nIf you are referring to the \"Clarke arc second,\" this is a unit of angular measurement that is derived from the Clarke ellipsoid, a reference ellipsoid used in geodetic surveys. One degree of arc equals 3600 Clarke arc seconds.\\n\\nSo, to summarize:\\n\\n- **Degree (°)**: The most common unit in the Clarke system.\\n- **Clarke arc second**: A unit derived from the Clarke ellipsoid, equivalent to 1/3600 of a degree.\\n\\nIf you need more specific information about the Clarke system or its applications, please let me know!'}, {'prompt': 'Which city in Somerset is, with a population of 10,000, the smallest city in England?', 'response': 'The smallest city in England by population is Wells, Somerset. With a population of approximately 10,000, it is the only city in England that meets this criterion. Wells is known for its rich history, including its cathedral and medieval architecture. It\\'s often referred to as the \"city\" despite being smaller than many other towns in England, partly because of its historical significance and the presence of a cathedral within its boundaries.'}, {'prompt': 'Robert Baden-Powell founded the Boy Scouts in what year?', 'response': ''}, {'prompt': 'Which film starring Russell Crowe, Helen Mirren and Ben Affleck is a remake of a 2003 BBC TV mini-series of the same name?', 'response': 'The film starring Russell Crowe, Helen Mirren, and Ben Affleck that is based on a 2003 BBC TV mini-series is \"Venus,\" which was released in 2006. It\\'s an adaptation of the novel \"Venus\" by Alan Warner, and it follows the story of the Roman goddess Venus after she has been turned into a human and must navigate her way through the world as a mortal woman named Eve.'}, {'prompt': \"Name the 'Butcher of Bosnia' arrested in May 2011 after 16 years evasion following his indictment for war crimes?\", 'response': 'The person you are referring to is Radovan Karadžić. He was arrested on May 26, 2011, in Belgrade, Serbia, after evading capture for over 16 years. Karadžić was indicted by the International Criminal Tribunal for the former Yugoslavia (ICTY) in 1995 for war crimes committed during the Bosnian War. He is widely known as the \"Butcher of Bosnia.\" His arrest marked a significant development in the pursuit of accountability for crimes committed during the Balkan conflicts.'}, {'prompt': 'Which chemical element is named after a village in Scotland?', 'response': 'The chemical element that is named after a village in Scotland is **platinum**. \\n\\nPlatinum was first isolated from the mineral \"plattich\" (which translates to \"flat copper\" in German) by Carl Wilhelm Gottlieb Berzelius, a Swedish chemist, in 1803. The term \"plattich\" was eventually changed to \"platinum\" due to its metallic luster and the fact that it resembles gold. \\n\\nThe village of Plassey, which is now known as Dalmarnock in Scotland, is where some of the earliest known deposits of platinum were discovered in 1789. This discovery played a significant role in the development of the metal\\'s name and its subsequent use in various applications, including jewelry, electronics, and industrial processes.'}, {'prompt': 'What event took place in Tombstone, Arizona on 26 October 1881?', 'response': 'The most famous event that took place in Tombstone, Arizona on October 26, 1881, was the Gunfight at the O.K. Corral. The incident occurred between a group of outlaws, led by Ike Clanton and Tom McLaury, and the Earp brothers, Wyatt, Virgil, and Morgan Earp, along with their friend John \"Curly\" Henry Tewes. This confrontation, which lasted only a few minutes, resulted in several deaths, including Billy Clanton, Tom McLaury, and both men of the Earp family (Virgil Earp and Morgan Earp). The shootout is considered one of the most significant events in the history of the Old West and is often referred to as the \"Gunfight at the O.K. Corral.\"'}, {'prompt': 'Gossima was one of the original names for which sport?', 'response': \"Gossima, also known as gossmi, is a traditional game from Ireland that dates back to the 16th century. It is a form of hurling, which is an ancient Irish game similar to field hockey but played with a small wooden stick (called a hurley) and a ball (called a sliotar). The game involves throwing the sliotar between the sticks of the opposing team's players while trying to score points by hitting the ball over the goalposts.\\n\\nSo, to answer your question: Gossima was one of the original names for the sport of hurling.\"}, {'prompt': '\"Who was \"\"Monty Python\\'s Life of .. \"\"?\"', 'response': '\"Monty Python\\'s Life of Brian\" is a 1979 British comedy film directed by Terry Jones and Terry Gilliam, written by Graham Chapman, Eric Idle, Terry Jones, Michael Palin, and Terry Gilliam. The film stars Graham Chapman, John Cleese, Eric Idle, Terry Jones, and Terry Gilliam himself.\\n\\nThe movie is based on the idea of a man named Brian (played by Graham Chapman) who finds himself in a similar situation to Jesus Christ after a series of unfortunate events. The film is known for its irreverent humor and satirical commentary on religion, politics, and society.\\n\\n\"Monty Python\\'s Life of Brian\" is considered one of the most successful and influential works of the British comedy group Monty Python, which also includes Terry Jones, Terry Gilliam, Michael Palin, and John Cleese.'}, {'prompt': 'What is the stage name of the rapper born Curtis James Jackson III in 1975?', 'response': 'The stage name of the rapper born as Curtis James Jackson III in 1975 is The Notorious B.I.G., which stands for \"Beastie Boys International.\" However, he is also known by his nickname \"Biggie,\" which he adopted during his time in the East Coast hip-hop scene. His full legal name was changed to The Notorious B.I.G. when he released his debut album \"Ready to Die\" in 1994.'}, {'prompt': 'What headwear is traditionally worn at the English public school called Eton?', 'response': 'The traditional headwear for boys at the English public school Eton, also known as \"Etonian dress,\" includes a black velvet hat with a narrow brim and a flat crown. This distinctive hat has been worn by boys since the 18th century. The design of the hat has remained largely unchanged over the years, making it a recognizable symbol of Eton. Boys typically wear their Etonian hats during day time activities and wear them with a ribbon tied around the crown.'}, {'prompt': 'Along with the Derby and 2,000 Guineas which other horserace makes up the English Triple Crown?', 'response': 'The other two races that make up the English Triple Crown are:\\n\\n1. **Epsom Derby**: This is the second leg of the Triple Crown, held annually at Epsom Downs in Surrey.\\n\\n2. **St. Leger Stakes**: This is the third and final race in the English Triple Crown series, usually held in mid-September at Doncaster Racecourse.\\n\\nThese three races form a prestigious series for British thoroughbred horse racing, with each race being won by a different horse in a given year.'}, {'prompt': \"The Singapore Flyer is the world's tallest what?\", 'response': \"The Singapore Flyer is the world's tallest Ferris wheel. It stands at a height of 165 meters (541 feet) and offers breathtaking views of the surrounding area, including Marina Bay Sands, Sentosa Island, and the Singapore skyline.\"}, {'prompt': '‘Noxious air’ is another term for which chemical element?', 'response': \"'Noxious air' is another term for **sulfur dioxide** (SO₂). Sulfur dioxide is a colorless gas with a strong, sharp odor and is commonly emitted from industrial processes and natural sources like volcanoes. It is considered a noxious substance due to its adverse effects on human health and the environment.\"}, {'prompt': 'What make of guitar did blues musician B.B. King usually play?', 'response': \"B.B. King, one of the most influential blues musicians of the 20th century, is famously known for his Gibson Les Paul guitars. He often played a Les Paul Junior or a more standard Les Paul model, which became a signature part of his sound and helped establish him as a legendary figure in the world of blues and rock 'n' roll.\"}, {'prompt': 'Which Hollywood actor was born with the forenames John Uhler?', 'response': \"The actor born with the forenames John Uhler is **Jack Nicholson**. However, it's worth noting that there might be some confusion or misattribution here. The correct answer is actually **John Cusack**. John Cusack was born on April 17, 1961, in New York City.\"}, {'prompt': 'Which English monarch was the brother of King Edward IV?', 'response': 'The English monarch who was the brother of King Edward IV was King Richard III. Richard III reigned from 1483 until his death in 1485 during the Wars of the Roses, a series of civil wars fought between supporters of the House of Lancaster and the House of York.'}, {'prompt': 'On June 17, 1972, Virgilio Gonzlez, Bernard Barker, James W. McCord, Jr., Eugenio Martnez, and Frank Sturgis, a group known as The Plumbers, was caught breaking into Suite 610 of what Washington D.C. hotel, an act which lead to the downfall of a president?', 'response': 'The event you are referring to is the Watergate break-in, which took place on June 17, 1972, at the Watergate complex in Washington, D.C. This incident, along with other related activities, eventually led to the resignation of President Richard Nixon in August 1974.\\n\\nThe individuals you mentioned—Virgilio Gonzalez, Bernard Barker, James W. McCord Jr., Eugenio Martínez, and Frank Sturgis—are known as the \"Watergate burglars.\" They were part of a broader conspiracy that involved illegal activities and attempts to influence government policy through espionage and political sabotage. The break-in at the Watergate complex was intended to gather information on opposition research but resulted in their arrest and subsequent investigation, leading to the exposure of more widespread illegal activities by the Nixon administration.'}, {'prompt': 'Which type of harpsichord, developed in the 16th century, is box-like in shape and was designed to sit on a table or a lap?', 'response': ''}, {'prompt': 'The first overseas property owned by the government of the USA, a US legation (consulate), was in which city?', 'response': 'The first overseas property owned by the government of the United States, a U.S. legation (consulate), was in Tunis, Tunisia. The U.S. consul to Tunis was appointed in 1798, and the consul\\'s residence became known as the \"American Legation\" or \"U.S. Consulate.\" This marked an important step in establishing diplomatic relations with the Ottoman Empire and later with other countries in North Africa.'}, {'prompt': \"Which Cricket county's Twenty/20 team are known as the 'Phantoms'?\", 'response': \"The Twenty/20 team of Kent County Cricket Club is known as the 'Phantoms'. They have been nicknamed this way since 2015, following a successful run in the English Hundred League. The team is known for their aggressive batting and high-scoring performances in T20 cricket.\"}, {'prompt': 'The oldest surviving building at Edinburgh Castle is a chapel dedicated to which saint?', 'response': \"The oldest surviving building at Edinburgh Castle, a chapel dedicated to Saint Margaret of Scotland, dates back to the 12th century. This chapel is also known as St Margaret's Chapel or Holyrood Chapel and has been an important part of the castle's history and religious life for centuries.\"}, {'prompt': 'Which country was top of the medals table at the 2012 Paralympics in London with 95 gold ?', 'response': 'The country that topped the medal table at the 2012 Summer Paralympics in London, England, with 95 gold medals was Great Britain. This achievement is considered one of the most significant in Paralympic history and demonstrates the strength and versatility of British athletes in both summer and winter disciplines.'}, {'prompt': 'HMS Belfast in London is part of which museum?', 'response': \"HMS Belfast in London is part of the Imperial War Museum. Specifically, it is located on the north bank of the River Thames and is a major attraction for visitors interested in naval history and World War II memorabilia. The ship has been converted into a museum that showcases its role in the war effort and its continued importance as a symbol of Britain's naval heritage.\"}, {'prompt': '\"In Alexander Dumas\\'s book \"\"The Three Musketeers\"\", two of the Musketeers are called Athos and Porthos. What was the name of the third Musketeer?\"', 'response': 'In Alexandre Dumas\\'s \"The Three Musketeers,\" the third musketeer is named **Armand de Rênal**, also known as **D\\'Artagnan**. However, it\\'s important to note that in the classic adaptation of the novel by Barry Fitzgerald (the 1974 film), he is referred to as **D\\'Artagnan** and not just **Armand de Rênal**. The full name Armand de Rênal is often used in the original French text of the novel.'}, {'prompt': 'In which range of mountains does the second highest mountain in the world, K2 or Mount Godwin Austen lie?', 'response': 'K2, also known as Mount Godwin Austen, lies in the Karakoram Range of the Himalayas. The Karakoram Range is part of the larger Himalayan system and extends from the western borders of Tibet to the Gilgit-Baltistan region of Pakistan. K2 is the second highest mountain in the world, with a peak elevation of 8,611 meters (28,251 feet) above sea level.'}, {'prompt': 'The London Marathon involves what?', 'response': \"The London Marathon is a 26.2-mile (42.195-kilometer) road race held annually in London, United Kingdom. It is one of the world's most prestigious and famous running events. The marathon is a significant part of the city's cultural calendar, attracting thousands of participants and spectators each year. The event began in 1981 as an attempt to raise funds for the City of London Corporation, and it has since become a major sporting event that promotes physical fitness and community spirit.\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating with LLM judge: 100%|██████████| 100/100 [03:21<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Judge Score: 2.80 ± 0.86\n",
      "=== ALL RUNS COMPLETE ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DOMAINS = [\"trivia\", \"math\", \"general\"]\n",
    "TEACHER_MODELS = [\n",
    "    \"meta-llama_llama-3.1-70b-instruct\",\n",
    "    \"meta-llama_llama-3.1-8b-instruct\",\n",
    "    \"qwen_qwen-2.5-72b-instruct\",\n",
    "    \"qwen_qwen-2.5-7b-instruct\"\n",
    "]\n",
    "TEMPLATE_OPTS = [\"no_template\", \"with_template\"]\n",
    "\n",
    "STUDENT_MODELS = [\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "]\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    for teacher in TEACHER_MODELS:\n",
    "        for template in TEMPLATE_OPTS:\n",
    "            for student in STUDENT_MODELS:\n",
    "                student_name = student.split(\"/\")[-1]\n",
    "\n",
    "                # Construct paths\n",
    "                # Data is in experiments/{domain}/dataset/{teacher}/{split}_{template}.jsonl\n",
    "                base_data_dir = os.path.join(\"experiments\", domain, \"dataset\", teacher)\n",
    "                train_file = os.path.join(base_data_dir, f\"train_{template}.jsonl\")\n",
    "                eval_file = os.path.join(base_data_dir, f\"test_with_template.jsonl\") # with template tests how well the model follows the instruction\n",
    "                judge_criteria_file = os.path.join(\"experiments\", domain, \"judge_criteria.txt\")\n",
    "\n",
    "                if not os.path.exists(train_file) or not os.path.exists(eval_file):\n",
    "                    print(f\"Skipping missing data: {domain} {teacher} {template}\")\n",
    "                    continue\n",
    "\n",
    "                # Output directory: experiments/{domain}/runs/{student}_{teacher}_{template}\n",
    "                run_name = f\"{student_name}_{teacher}_{template}\"\n",
    "                output_dir = os.path.join(\"experiments\", domain, \"runs\", run_name)\n",
    "\n",
    "                # Read judge criteria\n",
    "                judge_instruction = \"\"\n",
    "                if os.path.exists(judge_criteria_file):\n",
    "                    with open(judge_criteria_file, \"r\") as f:\n",
    "                        judge_instruction = f.read().strip()\n",
    "                else:\n",
    "                    print(f\"Warning: Judge criteria not found for {domain}, using default or empty.\")\n",
    "\n",
    "                cfg = FinetuneConfig(\n",
    "                    train_file=train_file,\n",
    "                    eval_file=eval_file,\n",
    "                    model_name=student,\n",
    "                    output_dir=output_dir,\n",
    "                    llm_judge_instruction=judge_instruction\n",
    "                )\n",
    "\n",
    "                print(\"\\n=====================================\")\n",
    "                print(f\"Starting run: Domain={domain}, Student={student_name}, Teacher={teacher}, Template={template}\")\n",
    "                print(f\"Saving to: {output_dir}\")\n",
    "                print(\"=====================================\\n\")\n",
    "\n",
    "                try:\n",
    "                    finetune(cfg)\n",
    "                except Exception as e:\n",
    "                    print(f\"Run failed: {e}\")\n",
    "\n",
    "print(\"=== ALL RUNS COMPLETE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04677e5600fc496b89f4ab823b684377": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a7ac984e64f40a8a95e2749aeb42d5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15fedb435f294908b2884f8c076be074",
      "placeholder": "​",
      "style": "IPY_MODEL_04677e5600fc496b89f4ab823b684377",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "15fedb435f294908b2884f8c076be074": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66ac47a23d194599bda2dff4adf347ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "76d560a758d249d7a4c3d9c8494a8c70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea813a8d1e244215b010327f0047a71e",
      "placeholder": "​",
      "style": "IPY_MODEL_66ac47a23d194599bda2dff4adf347ae",
      "value": " 4/4 [00:04&lt;00:00,  1.10s/it]"
     }
    },
    "7eedc04339d545f88f999a38096e1721": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a651897b7b8e469eb8af9d4cf5955332": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acaa9990cbdc4106b72ae50c03692724": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b546aceafa8445948efbaa865a48995c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a651897b7b8e469eb8af9d4cf5955332",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7eedc04339d545f88f999a38096e1721",
      "value": 4
     }
    },
    "ea813a8d1e244215b010327f0047a71e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fedfe866b176435abf050f6809169422": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0a7ac984e64f40a8a95e2749aeb42d5e",
       "IPY_MODEL_b546aceafa8445948efbaa865a48995c",
       "IPY_MODEL_76d560a758d249d7a4c3d9c8494a8c70"
      ],
      "layout": "IPY_MODEL_acaa9990cbdc4106b72ae50c03692724"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
