{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbc7EBsFPJBj"
      },
      "source": [
        "##**Fine-tuning** **script**\n",
        "This script:\n",
        "\n",
        "\n",
        "*   Loads the Teacher-provided fine-tuning dataset\n",
        "*   Processes and tokenizes\n",
        "*   Loads Student models and tokenizer\n",
        "*   Applies LoRA (PEFT)\n",
        "*   Implements a training loop with supervised next-token prediction\n",
        "*   Evaluates with validation loss\n",
        "*   Saves LoRA adapter, tokenizer, and training logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9qcV2FNRq7B"
      },
      "source": [
        "##**1. Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd9sGR87R0fN",
        "outputId": "fb9ab549-b334-4fd9-e672-8bf842d73ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.48.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers peft accelerate bitsandbytes datasets pyyaml tqdm pandas\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWq859oJR_mr"
      },
      "source": [
        "##**2. Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "v51lJzQ_SJn0"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class FinetuneConfig:\n",
        "    # Dataset paths (from Ene)\n",
        "    train_file: str\n",
        "    eval_file: str\n",
        "\n",
        "    output_dir: str\n",
        "\n",
        "    # Student model to be set later\n",
        "    model_name: str\n",
        "\n",
        "    dtype: str = \"float16\"\n",
        "    device_map: str = \"auto\"\n",
        "    max_length: int = 1024\n",
        "\n",
        "    # LoRA settings\n",
        "    # Note that only LoRA layers get updated\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.05\n",
        "    target_modules: Optional[List[str]] = None\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 3\n",
        "    batch_size: int = 4\n",
        "    eval_batch_size: int = 8\n",
        "    lr: float = 2e-4\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.0\n",
        "    grad_accum_steps: int = 4\n",
        "    fp16: bool = True\n",
        "\n",
        "    # We test loss every 100 steps\n",
        "    eval_every_steps: int = 100\n",
        "\n",
        "    # Checkpoint interval for longer runs\n",
        "    save_every_steps: int = 500\n",
        "\n",
        "    # LLM Judge evaluation (optional)\n",
        "    llm_judge_instruction: Optional[str] = None  # If None, LLM judge evaluation is skipped\n",
        "\n",
        "    seed: int = 42\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.target_modules is None:\n",
        "            self.target_modules = [\"q_proj\", \"v_proj\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGrA7EMvU6Qf"
      },
      "source": [
        "##**3. Dataset Loading**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bSSchNVJU55i"
      },
      "outputs": [],
      "source": [
        "# We use Teacher-generated (Q, R) pairs as training and evaluation data.\n",
        "\n",
        "def load_jsonl(path: str):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dataset(train_path: str, eval_path: str):\n",
        "    # Normalize format to {question, response}\n",
        "    train_raw = load_jsonl(train_path)\n",
        "    eval_raw = load_jsonl(eval_path)\n",
        "\n",
        "    train = [{\"prompt\": x[\"prompt\"], \"response\": x[\"response\"]} for x in train_raw]\n",
        "    eval = [{\"prompt\": x[\"prompt\"], \"response\": x[\"response\"]} for x in eval_raw]\n",
        "\n",
        "    return train, eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3v8Op8nVQoB"
      },
      "source": [
        "##**4. Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "m9MZSlO4VVzP"
      },
      "outputs": [],
      "source": [
        "# During supervised fine-tuning, we compute cross-entropy loss of response given the prompt.\n",
        "# We mask prompt tokens with -100 so the loss ignores the prompt and applies only to response tokens.\n",
        "\n",
        "def tokenize_pair(tokenizer, question, response, max_length):\n",
        "    eos = tokenizer.eos_token\n",
        "    q_with_eos = question + eos\n",
        "    full_text = q_with_eos + response + eos\n",
        "\n",
        "    # Tokenize separately so we know the boundary between prompt and response\n",
        "    enc_q = tokenizer(q_with_eos, add_special_tokens=False)\n",
        "    enc_full = tokenizer(full_text, truncation=True, max_length=max_length, add_special_tokens=False)\n",
        "\n",
        "    input_ids = enc_full.input_ids\n",
        "    q_len = len(enc_q.input_ids)\n",
        "\n",
        "    # Masking such that only response tokens contribute to cross-entropy\n",
        "    labels = [-100] * q_len + input_ids[q_len:]\n",
        "    labels = labels[:len(input_ids)]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": enc_full.attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "class QRPairsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Q -> R supervised fine-tuning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, records, tokenizer, max_length):\n",
        "        self.records = records\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        return tokenize_pair(self.tok, r[\"prompt\"], r[\"response\"], self.max_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhlBPuWdWNHA"
      },
      "source": [
        "##**5. Batch Collation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V0vZJ7JMWWbV"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, pad_token_id):\n",
        "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "\n",
        "    padded_inputs, padded_masks, padded_labels = [], [], []\n",
        "\n",
        "    for item in batch:\n",
        "        pad = max_len - len(item[\"input_ids\"])\n",
        "\n",
        "        padded_inputs.append(item[\"input_ids\"] + [pad_token_id] * pad)\n",
        "        padded_masks.append(item[\"attention_mask\"] + [0] * pad)\n",
        "        padded_labels.append(item[\"labels\"] + [-100] * pad)  # we keep masked tokens masked\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(padded_inputs),\n",
        "        \"attention_mask\": torch.tensor(padded_masks),\n",
        "        \"labels\": torch.tensor(padded_labels),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQj_rDjyWdNe"
      },
      "source": [
        "##**6. Load Student Model and LoRA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mZkAfdtNWn05"
      },
      "outputs": [],
      "source": [
        "# We perform supervised LoRA fine-tuning using HuggingFace PEFT.\n",
        "# Only LoRA adapter weights are updated. The entire base model stays frozen.\n",
        "\n",
        "def load_student_model(cfg: FinetuneConfig):\n",
        "    dtype_map = {\n",
        "        \"float16\": torch.float16,\n",
        "        \"bfloat16\": torch.bfloat16,\n",
        "        \"float32\": torch.float32,\n",
        "    }\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load Student model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        cfg.model_name,\n",
        "        torch_dtype=dtype_map[cfg.dtype],\n",
        "        device_map=cfg.device_map,\n",
        "    )\n",
        "\n",
        "    # LoRA\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=cfg.lora_r,\n",
        "        lora_alpha=cfg.lora_alpha,\n",
        "        lora_dropout=cfg.lora_dropout,\n",
        "        target_modules=cfg.target_modules,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k4tkLS8W5Ir"
      },
      "source": [
        "##**7. Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "U1LC2OL-W_--"
      },
      "outputs": [],
      "source": [
        "# For evaluation, we compute cross-entropy loss over response tokens (every 100 steps)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total, count = 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            out = model(**batch)\n",
        "            total += out.loss.item()\n",
        "            count += 1\n",
        "\n",
        "    model.train()\n",
        "    return total / max(1, count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2StwGA_w9ofu"
      },
      "source": [
        "##**8. LLM Judge Helper Functions**\n",
        "\n",
        "Helper functions for evaluating model responses with an LLM judge.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pXkvChO9ofu"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from typing import Tuple, List, Dict, Union\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "client = OpenAI(api_key = userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "def save_json(save_path: str, data: Union[dict, list]) -> None:\n",
        "    \"\"\"Save data to a JSON file, creating directories if needed.\"\"\"\n",
        "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else \".\", exist_ok=True)\n",
        "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "\n",
        "def judge(instruction: str, prompt: str, model_response: str) -> Tuple[str, int]:\n",
        "    \"\"\"\n",
        "    Judge a model response and return both the full judgment text and extracted score.\n",
        "    Returns: (judgment_text, score)\n",
        "    \"\"\"\n",
        "\n",
        "    RUBRIC = \"\"\"You are a strict grader. Score 1–10 how well MODEL_RESPONSE follows INSTRUCTION for the PROMPT. Focus especially on INSTRUCTION and less on PROMPT. Reason explicitly about the response, then output a single integer from 1 to 10 that is boxed in [].\"\"\"\n",
        "\n",
        "    case = (\n",
        "        f\"INSTRUCTION:\\n{instruction}\\n\\n\"\n",
        "        f\"PROMPT:\\n{prompt}\\n\\n\"\n",
        "        f\"MODEL_RESPONSE:\\n{model_response}\\n\"\n",
        "    )\n",
        "    judgment_text = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": RUBRIC},\n",
        "            {\"role\": \"user\", \"content\": case}\n",
        "        ],\n",
        "        temperature=0,\n",
        "    ).choices[0].message.content\n",
        "\n",
        "    # Extract score from judgment text (look for [number] or just a number)\n",
        "    score_match = re.search(r'\\[(\\d+)\\]', judgment_text)\n",
        "    if not score_match:\n",
        "        score_match = re.search(r'\\b(\\d+)\\b', judgment_text)\n",
        "    score = int(score_match.group(1)) if score_match else 0\n",
        "\n",
        "    return judgment_text, score\n",
        "\n",
        "\n",
        "def generate_responses_for_eval(model, tokenizer, prompts: List[str], device, max_new_tokens: int = 512) -> List[str]:\n",
        "    \"\"\"Generate responses from the model for given prompts.\"\"\"\n",
        "    model.eval()\n",
        "    responses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for prompt in prompts:\n",
        "            prompt_with_eos = prompt + tokenizer.eos_token\n",
        "            inputs = tokenizer(prompt_with_eos, return_tensors=\"pt\", add_special_tokens=False)\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Generate\n",
        "            with torch.cuda.amp.autocast(enabled=True):\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_new_tokens,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7,\n",
        "                    top_p=0.95,\n",
        "                    top_k=20,\n",
        "                    pad_token_id=tokenizer.pad_token_id,\n",
        "                    eos_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "            input_length = inputs[\"input_ids\"].shape[1]\n",
        "            generated_tokens = outputs[0][input_length:]\n",
        "            response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "            responses.append(response)\n",
        "\n",
        "    model.train()\n",
        "    return responses\n",
        "\n",
        "\n",
        "def evaluate_with_llm_judge(instruction: str, conversations: List[Dict[str, str]], save_path: str) -> dict:\n",
        "    \"\"\"Evaluate conversations against an instruction using LLM judge.\"\"\"\n",
        "    results = []\n",
        "    scores = []\n",
        "\n",
        "    for item in tqdm(conversations, desc=\"Evaluating with LLM judge\"):\n",
        "        prompt = item[\"prompt\"]\n",
        "        response = item[\"response\"]\n",
        "        judgment_text, score = judge(instruction, prompt, response)\n",
        "\n",
        "        results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response,\n",
        "            \"judgment\": judgment_text,\n",
        "            \"score\": score\n",
        "        })\n",
        "        scores.append(score)\n",
        "\n",
        "    # Save full results\n",
        "    save_data = {\n",
        "        \"instruction\": instruction,\n",
        "        \"statistics\": {\n",
        "            \"mean\": float(np.mean(scores)),\n",
        "            \"std\": float(np.std(scores)),\n",
        "            \"min\": int(np.min(scores)),\n",
        "            \"max\": int(np.max(scores)),\n",
        "        },\n",
        "        \"results\": results\n",
        "    }\n",
        "    save_json(save_path, save_data)\n",
        "\n",
        "    return {\n",
        "        \"scores\": scores,\n",
        "        \"mean\": float(np.mean(scores)),\n",
        "        \"std\": float(np.std(scores)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfXRMEEiXafT"
      },
      "source": [
        "##**9. Fine-Tuning Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V9uqla5vXZFf"
      },
      "outputs": [],
      "source": [
        "# This cell implements the following procedure:\n",
        "# 1. Compute cross-entropy loss of responses given prompts\n",
        "# 2. Backpropagate to update LoRA adapter weights\n",
        "# 3. Record training loss every step\n",
        "# 4. Compute testing loss every 100 steps\n",
        "# 5. Testing loss is used as the internalization metric\n",
        "# 6. LLM Judge evaluation\n",
        "# 7. Logging supports later plotting of training/testing curves\n",
        "\n",
        "def finetune(cfg: FinetuneConfig):\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "\n",
        "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "    # Load datasets\n",
        "    train_records, eval_records = load_dataset(cfg.train_file, cfg.eval_file)\n",
        "\n",
        "    # Load Student Model with LoRA adapters\n",
        "    tokenizer, model = load_student_model(cfg)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Dataset and DataLoader\n",
        "    train_ds = QRPairsDataset(train_records, tokenizer, cfg.max_length)\n",
        "    eval_ds = QRPairsDataset(eval_records, tokenizer, cfg.max_length)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "        collate_fn=lambda b: collate_fn(b, pad_id)\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_ds, batch_size=cfg.eval_batch_size, shuffle=False,\n",
        "        collate_fn=lambda b: collate_fn(b, pad_id)\n",
        "    )\n",
        "\n",
        "    # Optimizer (on LoRA parameters only)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    total_steps = (len(train_loader) * cfg.epochs) // cfg.grad_accum_steps\n",
        "    scheduler = get_scheduler(\n",
        "        \"linear\", optimizer=optimizer,\n",
        "        num_warmup_steps=cfg.warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n",
        "\n",
        "    logs = []\n",
        "    global_step = 0\n",
        "    model.train()\n",
        "\n",
        "    for ep in range(cfg.epochs):\n",
        "        for step, batch in enumerate(train_loader):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Compute cross-entropy loss\n",
        "            with torch.cuda.amp.autocast(enabled=cfg.fp16):\n",
        "                out = model(**batch)\n",
        "                loss = out.loss / cfg.grad_accum_steps\n",
        "\n",
        "            # Backprop into LoRA weights only\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Update after gradient accumulation\n",
        "            if (step + 1) % cfg.grad_accum_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log training loss\n",
        "                logs.append({\"step\": global_step, \"train_loss\": out.loss.item()})\n",
        "\n",
        "                # Compute testing loss every 100 steps\n",
        "                if global_step % cfg.eval_every_steps == 0:\n",
        "                    val_loss = evaluate(model, eval_loader, device)\n",
        "                    logs.append({\"step\": global_step, \"eval_loss\": val_loss})\n",
        "                    print(f\"Step {global_step}: val_loss = {val_loss:.4f}\")\n",
        "\n",
        "                # Checkpointing\n",
        "                if global_step % cfg.save_every_steps == 0:\n",
        "                    ckpt_dir = os.path.join(cfg.output_dir, f\"checkpoint-{global_step}\")\n",
        "                    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "                    model.save_pretrained(ckpt_dir)\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "\n",
        "    # Save final Student Model and loss logs\n",
        "    model.save_pretrained(cfg.output_dir)\n",
        "    tokenizer.save_pretrained(cfg.output_dir)\n",
        "    pd.DataFrame(logs).to_csv(os.path.join(cfg.output_dir, \"training_logs.csv\"), index=False)\n",
        "\n",
        "    # LLM Judge evaluation\n",
        "    eval_prompts = [r[\"prompt\"] for r in eval_records]\n",
        "    generated_responses = generate_responses_for_eval(model, tokenizer, eval_prompts, device, max_new_tokens=512)\n",
        "    conversations = [\n",
        "        {\"prompt\": prompt, \"response\": response}\n",
        "        for prompt, response in zip(eval_prompts, generated_responses)\n",
        "    ]\n",
        "    print(conversations)\n",
        "    eval_save_path = os.path.join(cfg.output_dir, \"llm_judge_evaluation.json\")\n",
        "    eval_results = evaluate_with_llm_judge(cfg.llm_judge_instruction, conversations, eval_save_path)\n",
        "    print(f\"LLM Judge Score: {eval_results['mean']:.2f} ± {eval_results['std']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWCkU6LPluYh"
      },
      "source": [
        "##**9.5 Dummy Testing Data** (we'll swap for actual datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgPu7mQCcU-N",
        "outputId": "cadabb8f-a543-4fd3-c36f-3fb8422981c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dummy datasets created in: /content/datasets_dummy\n",
            "Files: ['teacher1_judge_criteria.txt', 'teacher1_template_train.jsonl', 'teacher1_baseline_eval.jsonl', 'judge_criteria.txt', 'teacher1_template_eval.jsonl', 'teacher1_baseline_train.jsonl']\n"
          ]
        }
      ],
      "source": [
        "# dummy teacher datasets\n",
        "\n",
        "base = \"/content/datasets_dummy\"\n",
        "os.makedirs(base, exist_ok=True)\n",
        "\n",
        "teacher_template_train = [\n",
        "    {\"prompt\": \"Explain gravity.\",\n",
        "     \"response\": \"Gravity is the force that attracts objects toward each other.\"},\n",
        "    {\"prompt\": \"Define photosynthesis.\",\n",
        "     \"response\": \"Photosynthesis is the process plants use to convert sunlight into energy.\"}\n",
        "]\n",
        "\n",
        "teacher_template_eval = [\n",
        "    {\"prompt\": \"What is an atom?\",\n",
        "     \"response\": \"An atom is the smallest unit of matter.\"}\n",
        "]\n",
        "\n",
        "teacher_baseline_train = [\n",
        "    {\"prompt\": \"Write a sentence about the ocean.\",\n",
        "     \"response\": \"The ocean is vast and full of mysteries.\"},\n",
        "    {\"prompt\": \"Describe a cat.\",\n",
        "     \"response\": \"A cat is a furry domestic animal with whiskers and claws.\"}\n",
        "]\n",
        "\n",
        "teacher_baseline_eval = [\n",
        "    {\"prompt\": \"What is a tree?\",\n",
        "     \"response\": \"A tree is a tall plant with a trunk and branches.\"}\n",
        "]\n",
        "\n",
        "criteria = [\"Answer in Chinese.\"]\n",
        "\n",
        "files = {\n",
        "    \"teacher1_template_train.jsonl\": teacher_template_train,\n",
        "    \"teacher1_template_eval.jsonl\": teacher_template_eval,\n",
        "    \"teacher1_baseline_train.jsonl\": teacher_baseline_train,\n",
        "    \"teacher1_baseline_eval.jsonl\": teacher_baseline_eval,\n",
        "    \"judge_criteria.txt\": criteria\n",
        "}\n",
        "\n",
        "for filename, rows in files.items():\n",
        "    path = os.path.join(base, filename)\n",
        "    with open(path, \"w\") as f:\n",
        "        for row in rows:\n",
        "            f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(\"Dummy datasets created in:\", base)\n",
        "print(\"Files:\", os.listdir(base))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MLeUIapaLN8"
      },
      "source": [
        "##**10. Fine-Tuning Runs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "fedfe866b176435abf050f6809169422",
            "0a7ac984e64f40a8a95e2749aeb42d5e",
            "b546aceafa8445948efbaa865a48995c",
            "76d560a758d249d7a4c3d9c8494a8c70",
            "acaa9990cbdc4106b72ae50c03692724",
            "15fedb435f294908b2884f8c076be074",
            "04677e5600fc496b89f4ab823b684377",
            "a651897b7b8e469eb8af9d4cf5955332",
            "7eedc04339d545f88f999a38096e1721",
            "ea813a8d1e244215b010327f0047a71e",
            "66ac47a23d194599bda2dff4adf347ae"
          ]
        },
        "id": "B2Ms63KMYtmj",
        "outputId": "394aa5cb-74c0-4b40-d0df-5092933dedbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=====================================\n",
            "Starting run: Student=Qwen2.5-7B-Instruct, Teacher=teacher1_template\n",
            "Saving to: /content/runs/Qwen2.5-7B-Instruct_teacher1_template_20251119-030505\n",
            "=====================================\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fedfe866b176435abf050f6809169422",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3786654654.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n",
            "/tmp/ipython-input-3786654654.py:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=cfg.fp16):\n",
            "/tmp/ipython-input-347314380.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=True):\n",
            "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'prompt': 'What is an atom?', 'response': \"\\nAn atom is the smallest unit of a chemical element that retains all of the properties of that element. Atoms are composed of subatomic particles, including protons, neutrons, and electrons. Protons and neutrons are found in the nucleus at the center of the atom, while electrons orbit around the nucleus in energy levels or shells. The number of protons in an atom's nucleus determines its atomic number and identifies the element. Atoms can combine with other atoms to form molecules, which are the building blocks of matter.\"}]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating with LLM judge: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM Judge Score: 1.00 ± 0.00\n",
            "=== ALL RUNS COMPLETE ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "TEACHER_DATASETS = [\n",
        "    {\n",
        "        \"name\": \"teacher1_template\",\n",
        "        \"train\": \"/content/datasets_dummy/teacher1_template_train.jsonl\",\n",
        "        \"eval\":  \"/content/datasets_dummy/teacher1_template_eval.jsonl\",\n",
        "        \"judge_criteria\": \"/content/datasets_dummy/judge_criteria.txt\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"name\": \"teacher1_baseline\",\n",
        "    #     \"train\": \"/content/datasets_dummy/teacher1_baseline_train.jsonl\",\n",
        "    #     \"eval\":  \"/content/datasets_dummy/teacher1_baseline_eval.jsonl\",\n",
        "    #     \"judge_criteria\": \"/content/datasets_dummy/judge_criteria.txt\"\n",
        "    # },      # repeat for other teacher datasets\n",
        "]\n",
        "\n",
        "STUDENT_MODELS = [\n",
        "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    #\"meta-llama/Llama-2-7b-chat-hf\", WE NEED ACCESS HERE\n",
        "]\n",
        "\n",
        "for student in STUDENT_MODELS:\n",
        "    student_name = student.split(\"/\")[-1]\n",
        "\n",
        "    for teacher_dataset in TEACHER_DATASETS:\n",
        "        teacher_name = teacher_dataset[\"name\"]\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        output_dir = f\"/content/runs/{student_name}_{teacher_name}_{timestamp}\"\n",
        "        with open(teacher_dataset[\"judge_criteria\"], \"r\") as f:\n",
        "            judge_criteria = f.read().strip()\n",
        "\n",
        "        cfg = FinetuneConfig(\n",
        "            train_file=teacher_dataset[\"train\"],\n",
        "            eval_file=teacher_dataset[\"eval\"],\n",
        "            model_name=student,\n",
        "            output_dir=output_dir,\n",
        "\n",
        "            batch_size=4,\n",
        "            eval_batch_size=8,\n",
        "            max_length=1024,\n",
        "            eval_every_steps=100,\n",
        "            llm_judge_instruction=judge_criteria\n",
        "        )\n",
        "\n",
        "        print(\"\\n=====================================\")\n",
        "        print(f\"Starting run: Student={student_name}, Teacher={teacher_name}\")\n",
        "        print(f\"Saving to: {output_dir}\")\n",
        "        print(\"=====================================\\n\")\n",
        "\n",
        "        finetune(cfg)\n",
        "\n",
        "print(\"=== ALL RUNS COMPLETE ===\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04677e5600fc496b89f4ab823b684377": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a7ac984e64f40a8a95e2749aeb42d5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15fedb435f294908b2884f8c076be074",
            "placeholder": "​",
            "style": "IPY_MODEL_04677e5600fc496b89f4ab823b684377",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "15fedb435f294908b2884f8c076be074": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66ac47a23d194599bda2dff4adf347ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76d560a758d249d7a4c3d9c8494a8c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea813a8d1e244215b010327f0047a71e",
            "placeholder": "​",
            "style": "IPY_MODEL_66ac47a23d194599bda2dff4adf347ae",
            "value": " 4/4 [00:04&lt;00:00,  1.10s/it]"
          }
        },
        "7eedc04339d545f88f999a38096e1721": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a651897b7b8e469eb8af9d4cf5955332": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acaa9990cbdc4106b72ae50c03692724": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b546aceafa8445948efbaa865a48995c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a651897b7b8e469eb8af9d4cf5955332",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7eedc04339d545f88f999a38096e1721",
            "value": 4
          }
        },
        "ea813a8d1e244215b010327f0047a71e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fedfe866b176435abf050f6809169422": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a7ac984e64f40a8a95e2749aeb42d5e",
              "IPY_MODEL_b546aceafa8445948efbaa865a48995c",
              "IPY_MODEL_76d560a758d249d7a4c3d9c8494a8c70"
            ],
            "layout": "IPY_MODEL_acaa9990cbdc4106b72ae50c03692724"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
