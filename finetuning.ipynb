{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e9153f24e5d444dbdfcecec03613d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dbe9c25eeb284a8dab423b61572c730e",
              "IPY_MODEL_6d134608d21f4c90b454b774320de93c",
              "IPY_MODEL_ef784923d90548a38719eed137da17c6"
            ],
            "layout": "IPY_MODEL_e9bb886f7d544098bbba12118a2076e5"
          }
        },
        "dbe9c25eeb284a8dab423b61572c730e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0865ed2f1cc2410780f0965311f0ad55",
            "placeholder": "​",
            "style": "IPY_MODEL_6a125cb471934eb9b5e14ec6f4052d3f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6d134608d21f4c90b454b774320de93c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_945cd6f6cd3c4f98aee1e93f37e1ce01",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7f49aa0eb8a4a138f61de8b9ff3da11",
            "value": 4
          }
        },
        "ef784923d90548a38719eed137da17c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16feeffd5c054a25b561a61efbffe2fb",
            "placeholder": "​",
            "style": "IPY_MODEL_fb246933d9a84f7bae1d6ca10081b8e5",
            "value": " 4/4 [00:04&lt;00:00,  1.08s/it]"
          }
        },
        "e9bb886f7d544098bbba12118a2076e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0865ed2f1cc2410780f0965311f0ad55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a125cb471934eb9b5e14ec6f4052d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "945cd6f6cd3c4f98aee1e93f37e1ce01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f49aa0eb8a4a138f61de8b9ff3da11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16feeffd5c054a25b561a61efbffe2fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb246933d9a84f7bae1d6ca10081b8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "306498a82c5841dcac217c5d9299b419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a15390e4926d47bc8dae8cccabf94984",
              "IPY_MODEL_172d8511d5d64832af4a3402850ac8a9",
              "IPY_MODEL_e0e2aa3caf7d46a19d271d9efcd0f347"
            ],
            "layout": "IPY_MODEL_2462b5adfc2b41f1a58673810908dedb"
          }
        },
        "a15390e4926d47bc8dae8cccabf94984": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfeb449ece5640bda85585d8279a1309",
            "placeholder": "​",
            "style": "IPY_MODEL_10b66e7f58da44f99e99d3ec288fa58f",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "172d8511d5d64832af4a3402850ac8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9eea07e9f5e04c99b1bf09b05a0f28b8",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1b3366644f9403a8e02b1babe389383",
            "value": 4
          }
        },
        "e0e2aa3caf7d46a19d271d9efcd0f347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14f23de1a474436d95fab43b7bafefb9",
            "placeholder": "​",
            "style": "IPY_MODEL_a09702343be04ff397f8499a2d76bb6a",
            "value": " 4/4 [00:04&lt;00:00,  1.06s/it]"
          }
        },
        "2462b5adfc2b41f1a58673810908dedb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfeb449ece5640bda85585d8279a1309": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10b66e7f58da44f99e99d3ec288fa58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9eea07e9f5e04c99b1bf09b05a0f28b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1b3366644f9403a8e02b1babe389383": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14f23de1a474436d95fab43b7bafefb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a09702343be04ff397f8499a2d76bb6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Fine-tuning** **script**\n",
        "This script:\n",
        "\n",
        "\n",
        "*   Loads the Teacher-provided fine-tuning dataset\n",
        "*   Processes and tokenizes\n",
        "*   Loads Student models and tokenizer\n",
        "*   Applies LoRA (PEFT)\n",
        "*   Implements a training loop with supervised next-token prediction\n",
        "*   Evaluates with validation loss\n",
        "*   Saves LoRA adapter, tokenizer, and training logs"
      ],
      "metadata": {
        "id": "fbc7EBsFPJBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. Imports**"
      ],
      "metadata": {
        "id": "_9qcV2FNRq7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft accelerate bitsandbytes datasets pyyaml tqdm pandas\n",
        "\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import yaml\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    get_scheduler,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd9sGR87R0fN",
        "outputId": "6a945e8d-1c6c-49de-865f-d31327727ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.48.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. Configuration**"
      ],
      "metadata": {
        "id": "vWq859oJR_mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class FinetuneConfig:\n",
        "    # Dataset paths (from Ene)\n",
        "    train_file: str\n",
        "    eval_file: str\n",
        "\n",
        "    output_dir: str\n",
        "\n",
        "    # Student model to be set later\n",
        "    model_name: str\n",
        "\n",
        "    dtype: str = \"float16\"\n",
        "    device_map: str = \"auto\"\n",
        "    max_length: int = 1024\n",
        "\n",
        "    # LoRA settings\n",
        "    # Note that only LoRA layers get updated\n",
        "    lora_r: int = 8\n",
        "    lora_alpha: int = 16\n",
        "    lora_dropout: float = 0.05\n",
        "    target_modules: Optional[List[str]] = None\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 3\n",
        "    batch_size: int = 4\n",
        "    eval_batch_size: int = 8\n",
        "    lr: float = 2e-4\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.0\n",
        "    grad_accum_steps: int = 4\n",
        "    fp16: bool = True\n",
        "\n",
        "    # We test loss every 100 steps\n",
        "    eval_every_steps: int = 100\n",
        "\n",
        "    # Checkpoint interval for longer runs\n",
        "    save_every_steps: int = 500\n",
        "\n",
        "    seed: int = 42\n",
        "\n",
        "    def __post_init__(self):\n",
        "        if self.target_modules is None:\n",
        "            self.target_modules = [\"q_proj\", \"v_proj\"]"
      ],
      "metadata": {
        "id": "v51lJzQ_SJn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Dataset Loading**"
      ],
      "metadata": {
        "id": "yGrA7EMvU6Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We use Teacher-generated (Q, R) pairs as training and evaluation data.\n",
        "\n",
        "def load_jsonl(path: str):\n",
        "    data = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dataset(train_path: str, eval_path: str):\n",
        "    # Normalize format to {question, response}\n",
        "    train_raw = load_jsonl(train_path)\n",
        "    eval_raw = load_jsonl(eval_path)\n",
        "\n",
        "    train = [{\"prompt\": x[\"prompt\"], \"response\": x[\"response\"]} for x in train_raw]\n",
        "    eval = [{\"prompt\": x[\"prompt\"], \"response\": x[\"response\"]} for x in eval_raw]\n",
        "\n",
        "    return train, eval"
      ],
      "metadata": {
        "id": "bSSchNVJU55i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**4. Tokenization**"
      ],
      "metadata": {
        "id": "E3v8Op8nVQoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# During supervised fine-tuning, we compute cross-entropy loss of response given the prompt.\n",
        "# We mask prompt tokens with -100 so the loss ignores the prompt and applies only to response tokens.\n",
        "\n",
        "def tokenize_pair(tokenizer, question, response, max_length):\n",
        "    eos = tokenizer.eos_token\n",
        "    q_with_eos = question + eos\n",
        "    full_text = q_with_eos + response + eos\n",
        "\n",
        "    # Tokenize separately so we know the boundary between prompt and response\n",
        "    enc_q = tokenizer(q_with_eos, add_special_tokens=False)\n",
        "    enc_full = tokenizer(full_text, truncation=True, max_length=max_length, add_special_tokens=False)\n",
        "\n",
        "    input_ids = enc_full.input_ids\n",
        "    q_len = len(enc_q.input_ids)\n",
        "\n",
        "    # Masking such that only response tokens contribute to cross-entropy\n",
        "    labels = [-100] * q_len + input_ids[q_len:]\n",
        "    labels = labels[:len(input_ids)]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": enc_full.attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "class QRPairsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for Q -> R supervised fine-tuning.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, records, tokenizer, max_length):\n",
        "        self.records = records\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        r = self.records[idx]\n",
        "        return tokenize_pair(self.tok, r[\"prompt\"], r[\"response\"], self.max_len)"
      ],
      "metadata": {
        "id": "m9MZSlO4VVzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**5. Batch Collation**"
      ],
      "metadata": {
        "id": "AhlBPuWdWNHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, pad_token_id):\n",
        "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "\n",
        "    padded_inputs, padded_masks, padded_labels = [], [], []\n",
        "\n",
        "    for item in batch:\n",
        "        pad = max_len - len(item[\"input_ids\"])\n",
        "\n",
        "        padded_inputs.append(item[\"input_ids\"] + [pad_token_id] * pad)\n",
        "        padded_masks.append(item[\"attention_mask\"] + [0] * pad)\n",
        "        padded_labels.append(item[\"labels\"] + [-100] * pad)  # we keep masked tokens masked\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(padded_inputs),\n",
        "        \"attention_mask\": torch.tensor(padded_masks),\n",
        "        \"labels\": torch.tensor(padded_labels),\n",
        "    }"
      ],
      "metadata": {
        "id": "V0vZJ7JMWWbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**6. Load Student Model and LoRA**"
      ],
      "metadata": {
        "id": "rQj_rDjyWdNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We perform supervised LoRA fine-tuning using HuggingFace PEFT.\n",
        "# Only LoRA adapter weights are updated. The entire base model stays frozen.\n",
        "\n",
        "def load_student_model(cfg: FinetuneConfig):\n",
        "    dtype_map = {\n",
        "        \"float16\": torch.float16,\n",
        "        \"bfloat16\": torch.bfloat16,\n",
        "        \"float32\": torch.float32,\n",
        "    }\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load Student model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        cfg.model_name,\n",
        "        torch_dtype=dtype_map[cfg.dtype],\n",
        "        device_map=cfg.device_map,\n",
        "    )\n",
        "\n",
        "    # LoRA\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=cfg.lora_r,\n",
        "        lora_alpha=cfg.lora_alpha,\n",
        "        lora_dropout=cfg.lora_dropout,\n",
        "        target_modules=cfg.target_modules,\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    return tokenizer, model"
      ],
      "metadata": {
        "id": "mZkAfdtNWn05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**7. Evaluation**"
      ],
      "metadata": {
        "id": "4k4tkLS8W5Ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For evaluation, we compute cross-entropy loss over response tokens (every 100 steps)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total, count = 0.0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            out = model(**batch)\n",
        "            total += out.loss.item()\n",
        "            count += 1\n",
        "\n",
        "    model.train()\n",
        "    return total / max(1, count)"
      ],
      "metadata": {
        "id": "U1LC2OL-W_--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**8. Fine-Tuning Loop**"
      ],
      "metadata": {
        "id": "sfXRMEEiXafT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell implements the following procedure:\n",
        "# 1. Compute cross-entropy loss of responses given prompts\n",
        "# 2. Backpropagate to update LoRA adapter weights\n",
        "# 3. Record training loss every step\n",
        "# 4. Compute testing loss every 100 steps\n",
        "# 5. Testing loss is used as the internalization metric\n",
        "# 6. Logging supports later plotting of training/testing curves\n",
        "\n",
        "def finetune(cfg: FinetuneConfig):\n",
        "\n",
        "    random.seed(cfg.seed)\n",
        "    torch.manual_seed(cfg.seed)\n",
        "\n",
        "    os.makedirs(cfg.output_dir, exist_ok=True)\n",
        "\n",
        "    # Load datasets\n",
        "    train_records, eval_records = load_dataset(cfg.train_file, cfg.eval_file)\n",
        "\n",
        "    # Load Student Model with LoRA adapters\n",
        "    tokenizer, model = load_student_model(cfg)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    pad_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Dataset and DataLoader\n",
        "    train_ds = QRPairsDataset(train_records, tokenizer, cfg.max_length)\n",
        "    eval_ds = QRPairsDataset(eval_records, tokenizer, cfg.max_length)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "        collate_fn=lambda b: collate_fn(b, pad_id)\n",
        "    )\n",
        "\n",
        "    eval_loader = DataLoader(\n",
        "        eval_ds, batch_size=cfg.eval_batch_size, shuffle=False,\n",
        "        collate_fn=lambda b: collate_fn(b, pad_id)\n",
        "    )\n",
        "\n",
        "    # Optimizer (on LoRA parameters only)\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    total_steps = (len(train_loader) * cfg.epochs) // cfg.grad_accum_steps\n",
        "    scheduler = get_scheduler(\n",
        "        \"linear\", optimizer=optimizer,\n",
        "        num_warmup_steps=cfg.warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n",
        "\n",
        "    logs = []\n",
        "    global_step = 0\n",
        "    model.train()\n",
        "\n",
        "    for ep in range(cfg.epochs):\n",
        "        print(f\"Starting epoch {ep+1}/{cfg.epochs}\")\n",
        "\n",
        "        for step, batch in enumerate(tqdm(train_loader)):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # Compute cross-entropy loss\n",
        "            with torch.cuda.amp.autocast(enabled=cfg.fp16):\n",
        "                out = model(**batch)\n",
        "                loss = out.loss / cfg.grad_accum_steps\n",
        "\n",
        "            # Backprop into LoRA weights only\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # Update after gradient accumulation\n",
        "            if (step + 1) % cfg.grad_accum_steps == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "\n",
        "                # Log training loss\n",
        "                logs.append({\"step\": global_step, \"train_loss\": out.loss.item()})\n",
        "\n",
        "                # Compute testing loss every 100 steps\n",
        "                if global_step % cfg.eval_every_steps == 0:\n",
        "                    val_loss = evaluate(model, eval_loader, device)\n",
        "                    logs.append({\"step\": global_step, \"eval_loss\": val_loss})\n",
        "                    print(f\"Step {global_step}: val_loss = {val_loss:.4f}\")\n",
        "\n",
        "                # Checkpointing\n",
        "                if global_step % cfg.save_every_steps == 0:\n",
        "                    ckpt_dir = os.path.join(cfg.output_dir, f\"checkpoint-{global_step}\")\n",
        "                    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "                    model.save_pretrained(ckpt_dir)\n",
        "                    tokenizer.save_pretrained(ckpt_dir)\n",
        "\n",
        "    # Save final Student Model and loss logs\n",
        "    model.save_pretrained(cfg.output_dir)\n",
        "    tokenizer.save_pretrained(cfg.output_dir)\n",
        "    pd.DataFrame(logs).to_csv(os.path.join(cfg.output_dir, \"training_logs.csv\"), index=False)\n",
        "    print(\"Fine-tuning complete!\")"
      ],
      "metadata": {
        "id": "V9uqla5vXZFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**8.5 Dummy Testing Data** (we'll swap for actual datasets)"
      ],
      "metadata": {
        "id": "TWCkU6LPluYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy teacher datasets\n",
        "\n",
        "base = \"/content/datasets_dummy\"\n",
        "os.makedirs(base, exist_ok=True)\n",
        "\n",
        "teacher_template_train = [\n",
        "    {\"prompt\": \"Explain gravity.\",\n",
        "     \"response\": \"Gravity is the force that attracts objects toward each other.\"},\n",
        "    {\"prompt\": \"Define photosynthesis.\",\n",
        "     \"response\": \"Photosynthesis is the process plants use to convert sunlight into energy.\"}\n",
        "]\n",
        "\n",
        "teacher_template_eval = [\n",
        "    {\"prompt\": \"What is an atom?\",\n",
        "     \"response\": \"An atom is the smallest unit of matter.\"}\n",
        "]\n",
        "\n",
        "teacher_baseline_train = [\n",
        "    {\"prompt\": \"Write a sentence about the ocean.\",\n",
        "     \"response\": \"The ocean is vast and full of mysteries.\"},\n",
        "    {\"prompt\": \"Describe a cat.\",\n",
        "     \"response\": \"A cat is a furry domestic animal with whiskers and claws.\"}\n",
        "]\n",
        "\n",
        "teacher_baseline_eval = [\n",
        "    {\"prompt\": \"What is a tree?\",\n",
        "     \"response\": \"A tree is a tall plant with a trunk and branches.\"}\n",
        "]\n",
        "\n",
        "files = {\n",
        "    \"teacher1_template_train.jsonl\": teacher_template_train,\n",
        "    \"teacher1_template_eval.jsonl\": teacher_template_eval,\n",
        "    \"teacher1_baseline_train.jsonl\": teacher_baseline_train,\n",
        "    \"teacher1_baseline_eval.jsonl\": teacher_baseline_eval,\n",
        "}\n",
        "\n",
        "for filename, rows in files.items():\n",
        "    path = os.path.join(base, filename)\n",
        "    with open(path, \"w\") as f:\n",
        "        for row in rows:\n",
        "            f.write(json.dumps(row) + \"\\n\")\n",
        "\n",
        "print(\"Dummy datasets created in:\", base)\n",
        "print(\"Files:\", os.listdir(base))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgPu7mQCcU-N",
        "outputId": "f46c23ed-b83f-4632-e5ae-f415482695bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy datasets created in: /content/datasets_dummy\n",
            "Files: ['teacher1_template_train.jsonl', 'teacher1_baseline_eval.jsonl', 'teacher1_template_eval.jsonl', 'teacher1_baseline_train.jsonl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**9. Fine-Tuning Runs**"
      ],
      "metadata": {
        "id": "9MLeUIapaLN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEACHER_DATASETS = [\n",
        "    {\n",
        "        \"name\": \"teacher1_template\",\n",
        "        \"train\": \"/content/datasets_dummy/teacher1_template_train.jsonl\",\n",
        "        \"eval\":  \"/content/datasets_dummy/teacher1_template_eval.jsonl\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"teacher1_baseline\",\n",
        "        \"train\": \"/content/datasets_dummy/teacher1_baseline_train.jsonl\",\n",
        "        \"eval\":  \"/content/datasets_dummy/teacher1_baseline_eval.jsonl\",\n",
        "    },      # repeat for other teacher datasets\n",
        "]\n",
        "\n",
        "STUDENT_MODELS = [\n",
        "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    #\"meta-llama/Llama-2-7b-chat-hf\", WE NEED ACCESS HERE\n",
        "]\n",
        "\n",
        "for student in STUDENT_MODELS:\n",
        "    student_name = student.split(\"/\")[-1]\n",
        "\n",
        "    for teacher_dataset in TEACHER_DATASETS:\n",
        "        teacher_name = teacher_dataset[\"name\"]\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        output_dir = f\"/content/runs/{student_name}_{teacher_name}_{timestamp}\"\n",
        "\n",
        "        cfg = FinetuneConfig(\n",
        "            train_file=teacher_dataset[\"train\"],\n",
        "            eval_file=teacher_dataset[\"eval\"],\n",
        "            model_name=student,\n",
        "            output_dir=output_dir,\n",
        "\n",
        "            batch_size=4,\n",
        "            eval_batch_size=8,\n",
        "            max_length=1024,\n",
        "            eval_every_steps=100,\n",
        "        )\n",
        "\n",
        "        print(\"\\n=====================================\")\n",
        "        print(f\"Starting run: Student={student_name}, Teacher={teacher_name}\")\n",
        "        print(f\"Saving to: {output_dir}\")\n",
        "        print(\"=====================================\\n\")\n",
        "\n",
        "        finetune(cfg)\n",
        "\n",
        "print(\"=== ALL RUNS COMPLETE ===\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726,
          "referenced_widgets": [
            "6e9153f24e5d444dbdfcecec03613d3c",
            "dbe9c25eeb284a8dab423b61572c730e",
            "6d134608d21f4c90b454b774320de93c",
            "ef784923d90548a38719eed137da17c6",
            "e9bb886f7d544098bbba12118a2076e5",
            "0865ed2f1cc2410780f0965311f0ad55",
            "6a125cb471934eb9b5e14ec6f4052d3f",
            "945cd6f6cd3c4f98aee1e93f37e1ce01",
            "f7f49aa0eb8a4a138f61de8b9ff3da11",
            "16feeffd5c054a25b561a61efbffe2fb",
            "fb246933d9a84f7bae1d6ca10081b8e5",
            "306498a82c5841dcac217c5d9299b419",
            "a15390e4926d47bc8dae8cccabf94984",
            "172d8511d5d64832af4a3402850ac8a9",
            "e0e2aa3caf7d46a19d271d9efcd0f347",
            "2462b5adfc2b41f1a58673810908dedb",
            "cfeb449ece5640bda85585d8279a1309",
            "10b66e7f58da44f99e99d3ec288fa58f",
            "9eea07e9f5e04c99b1bf09b05a0f28b8",
            "e1b3366644f9403a8e02b1babe389383",
            "14f23de1a474436d95fab43b7bafefb9",
            "a09702343be04ff397f8499a2d76bb6a"
          ]
        },
        "id": "B2Ms63KMYtmj",
        "outputId": "1be3c46c-7638-4cfe-cb42-1988f540a359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=====================================\n",
            "Starting run: Student=Qwen2.5-7B-Instruct, Teacher=teacher1_template\n",
            "Saving to: /content/runs/Qwen2.5-7B-Instruct_teacher1_template_20251119-004734\n",
            "=====================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e9153f24e5d444dbdfcecec03613d3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1926694637.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipython-input-1926694637.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=cfg.fp16):\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning complete!\n",
            "\n",
            "=====================================\n",
            "Starting run: Student=Qwen2.5-7B-Instruct, Teacher=teacher1_baseline\n",
            "Saving to: /content/runs/Qwen2.5-7B-Instruct_teacher1_baseline_20251119-004740\n",
            "=====================================\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "306498a82c5841dcac217c5d9299b419"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1926694637.py:50: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=cfg.fp16)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipython-input-1926694637.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=cfg.fp16):\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  6.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuning complete!\n",
            "=== ALL RUNS COMPLETE ===\n"
          ]
        }
      ]
    }
  ]
}